---
title: "PROYECTO FINAL: AJUSTE DE MODELOS NO-LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
toc: yes
---


################# SPOILER: Gilfoyle deja la empresa

\clearpage

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")

```

#Clasificación.

## Comprensión del problema a resolver.


```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #LECTURA DE LOS DATOS
    har.train <- read.table("./UCIHARDataset/train/X_train.txt", sep="", head = F)
    lhar.train <- read.table("./UCIHARDataset/train/y_train.txt", sep="", head = F)
    har.data.train <- cbind(lhar.train, har.train)
    
    har.test <- read.table("./UCIHARDataset/test/X_test.txt", sep="", head = F)
    lhar.test <- read.table("./UCIHARDataset/test/y_test.txt", sep="", head = F)
    har.data.test <- cbind(lhar.test, har.test)


    summary(har.train[,1:10])
```

## Preprocesado de datos.
- Los datos están normalizados y escalados [-1,1]
- Para los métodos que utilizan métricas tenemos que quitar asimetrías.

### Modificación de los atributos cualitativos.
- No los haylos.


```{r echo = FALSE, warning=FALSE, results = FALSE, message = FALSE}
    #PREPROCESAMIENTO DE LOS DATOS
```


### Tratamiento de la asimetría con BoxCox.
<!--
Como ya hemos comentado anteriormente, hay varios atributos que presentan una alta asimetría, lo cual podría hacer que los métodos de predicción que apliquemos a continuación obtengan resultados peores. Para solucionar esto, utilizaremos un método llamado BoxCox. Este método se basa en la transformación potencial según un valor ($\lambda$) para aumentar la correlación entre las variables. Para elegir la mejor potencia (mejor $\lambda$), se busca entre los $\lambda$ que proporcionen un menor error residual. Aunque en la práctica, esto se realizará de manera automátcica con la función *preProcess()* vamos a ver cómo funciona en el caso de un atributo. 
Para que se vea mejor el funcionamiento, vamos a elegir el atributo que presente una mayor asimetría. Para ello, ordenamos los atributos en función de su asimetría:
-->

- Hay que quitar asimetría.

```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    har_asymmetry <- apply(har.train, 2, skewness)
    har_asymmetry <- sort(abs(har_asymmetry), decreasing = T)
    print(head(har_asymmetry))
    
```


### Llamada al método PreProcess.
<!--
Una vez entendidas las modificaciones que vamos a realizar a los datos, utilizaremos el método preProcess que se encarga de realizar todas estas modificaciones sobre el conjunto de datos pasado como argumento. Como ya hemos comentado, no vamos a aplicar el método *PCA*, luego la llamada quedaría de la siguiente forma:
-->

Llamamos al método PreProcess con Boxcox: quitar asimetría, PCA: para eliminar atributos

```{r, echo=T, warning=F}

ObjetoTrans <- preProcess(har.train,method = c("pca", "BoxCox", "center", "scale"), thres = 0.8)
#0.95 -> 102 componentes
#0.9 -> 63 componentes
#0.85 -> 40 componentes
#0.8 -> 26 componentes
#0.75 -> 16 componentes
#0.7 -> 10 componentes

har.trans.train <- predict(ObjetoTrans, har.train)
har.trans.test <- predict(ObjetoTrans, har.test)

```

## Conjuntos de validación, training y test usados.

A continuación pasaremos a explorar los distintos modelos sobre los que resolver el problema de clasificación para nuestro conjunto de datos. El procedimiento de validación que usaremos consistirá en tomar múltiples veces distintos conjuntos de entrenamiento sobre nuestro conjunto de datos (una vez transformados), con los que aprenderemos el modelo. Usaremos el resto del conjunto como test para evaluar cómo de bien predice el modelo aprendido con nuevos datos. Las proporciones utilizadas serán del 70 % de los datos para train, y el 30 % para test.

```{r, echo=F, warning=F}
  #Paso 5: Preparación de conjunto de train/test, y de las etiquetas train/test
  har.train <- har.trans.train  
  har.test <- har.trans.test
  
```



```{r, echo=F,warning=F}
# Funciones para el cálculo del error
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

# Función para evaluar loserrores para un conjunto de modelos.
# Argumentos:
# models - lista de modelos
# test - conjunto de test
# ltrain - etiquetas de train
# ltest - etiquetas de test
# Devuelve: Matriz de num_modelos x 2. Las columnas representan (Ein, Etest)
testFamilies <- function(models, test, ltrain, ltest){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```


## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por la función `glm` (Generalized Linear Models) de `R`. Para cada familia, siempre que admitan, utilizaremos distintas funciones de enlace. Las funciones de enlace nos permiten establecer una relación entre la media de la respuesta y los predictores del modelo. Las familias que vamos a considerar para el ajuste son:

- **Binomial**, con link **logit**. Regresión logística.
- **Binomial**, con link **probit**. Modelo binomial, con función de enlace $\Phi^{-1}(\mu)$, donde $\Phi$ es la distribución acumulada de la distribución normal.
- **Binomial**, con link **cauchit**. Modelo binomial, cuya función de enlace es la análoga a la del modelo anterior sobre una distribución de Cauhy, en lugar de la normal.
- **Gaussiana**, con link **identity**. Regresión lineal.
- **Gaussiana**, con link **log**. Distribución normal con función de enlace logarítmica.
- **Poisson**. Distribución de Poisson. 
- **Quasi**. Este modelo no tiene una varianza determinada como en el resto de familias. Indicaremos la especificación de varianza `"constant"`
- **Quasibinomial.** Distribución binomial, con la única diferencia de que no fija el parámetro de dispersión (intenta describir varianza adicional en los datos que no puede ser explicada mediant una distribución binomial).
- **Quasipoisson.** Distribución de Poisson, con la única diferencia de que no fija el parámetro de dispersion.



```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

testFamilies <- function(models, test, ltrain, ltest, modelNames = NULL){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  if(!is.null(modelNames)){
    rownames(results) <- modelNames
  }
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```

```{r, echo=F,warning=F}
validateLinearModels <- function(train,test,ltrain,ltest){
  count_1 = sum(ltrain==1)
  count_0 = sum(ltrain==0)
  count = count_1 + count_0
  
  if(count_1/count <= 0.1 || count_0/count <= 0.1){
      #cosas()
  }
  
  #Modelos
  #binomial
  print("Evaluando Binomial - Logit...")
  ml.binomial1 <- glm(ltrain ~ ., family = binomial(logit), data = train)
  print("Evaluando Binomial - Probit...")
  ml.binomial2 <- glm(ltrain ~ ., family = binomial(probit), data = train)
  print("Evaluando Binomial - Cauchit...")
  ml.binomial3 <- glm(ltrain ~ ., family = binomial(cauchit), data = train)
  #gaussiano
  print("Evaluando Gaussian - Identity...")
  ml.gaussian1 <- glm(ltrain ~ ., family = gaussian(identity), data = train)
  print("Evaluando Gaussian - Log...")
  ml.gaussian2 <- glm(ltrain ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
  #poisson
  print("Evaluando Poisson - Log...")
  ml.poisson1 <- glm(ltrain ~ ., family = poisson(log), data = train)
  
  #quasi
  print("Evaluando Quasi - Identity...")
  ml.quasi1 <- glm(ltrain ~ ., family = quasi(link = "identity", variance = "constant"), data = train)

  #quasibinomial
  print("Evaluando Quasibinomial...")
  ml.quasibinomial1 <- glm(ltrain ~ ., family = quasibinomial(link = "logit"), data = train)
  #quasipoisson
  print("Evaluando Quasipoisson...")
  ml.quasipoisson1 <- glm(ltrain ~ ., family = quasipoisson(link = "log"), data = train)  

  #models <- list(ml.gaussian2,ml.poisson1)
  models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
    #modelNames <- c("Gaussian","Poisson")
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  testFamilies(models, test, ltrain, ltest, modelNames)
}


```


Una vez definidos los modelos y las funciones a usar, procedemos al ajuste de los distintos modelos y al análisis de sus errores:

### 1, no 1



```{r, echo=F, warning=F}
# Comprobamos qué método proporciona un menor error de validación cruzada
#1, no 1
lhar.train.1 <- as.vector(lhar.train[,1])
lhar.test.1 <- as.vector(lhar.test[,1])
lhar.train.1[lhar.train.1 != 1] <- 0
lhar.test.1[lhar.test.1 != 1] <- 0

validateLinearModels(train = har.train, test = har.test, ltrain = lhar.train.1, ltest = lhar.test.1)


# Confirmamos que el modelo que mejor generaliza es el de Poisson
#ml.sahd <- glm(chd ~ ., family = poisson(log), data = sahd.data, subset=train, start=rep(0, ncol(sahd.data)+1))


```

```{r, echo=F,warning=F}
validateLinearAllLabels <- function(train,test,ltrain,ltest){
    output <- matrix(nrow = 9, ncol = 12)
    rownames(output) <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
    colnames(output) <- c("Ein 1", "Eout 1","Ein 2", "Eout 2","Ein 3", "Eout 3","Ein 4", "Eout 4","Ein 5", "Eout 5","Ein 6", "Eout 6")
    for( i in 1:6){
       cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       M <- validateLinearModels(train,test,ltrain_i,ltest_i)
       
       output[,c(2*i-1,2*i)] <- M[,c(1,2)]
    } 
    output
}
  
```

```{r, echo=F,warning=F}
validateLinearAllLabels(har.train,har.test,as.vector(lhar.train[,1]),as.vector(lhar.test[,1]))

```

Obtenemos que el modelo que mejores resultados proporciona es el de Poisson. Hay que destacar que los resultados de Poisson coinciden con los de quasipoisson, pero elegimos el de Poisson por ser más conocido.

## Regularización

A continuación nos planteamos la necesidad de regularización, sobre el mejor modelo que hemos obtenido. Para ello utilizamos la regularización lasso (least absolute shrinkage and selection operator). Lasso es un método que lleva a cabo la regularización a la misma vez que realiza selección de características. 

El objetivo se basa en reducir el error de predicción, para ello, se ocupa de reducir la función:

\[
  R(\beta) = \sum_{i=1}^{n}(y_i - x_i\omega)^2 + \lambda \sum_{i=1}^{p}|\omega_i|
\]

donde $n$ es el número de muestras, $p$ el número de atributos y $w$ el vector de pesos solución. Así, obtiene diferentes valores de $\lambda$. Entre estos valores devueltos, podemos considerar dos de ellos:

- *lambda.min*, que nos devuelve el valor del $\lambda$ para el que se minimiza el error obtenido.
- *lambda.1se*. Aunque el $\lambda$ anterior sea menor, presenta una mayor dependencia de las particiones seleccionadas en la validación cruzada. La regla `1se` (one standard error), elige un valor de $\lambda$ para lo suficiente cercano a `lambda.min` para el cual el modelo obtenido sea lo suficiente simple, reduciendo así la dependencia del error respecto a las validaciones.

Podemos contemplar cualquiera de estos valores de $\lambda$ para regularizar nuestro modelo. 

Para contestar a la pregunta de si era necesario aplicar regularización a nuestra base de datos, realizamos 100 experimentos en los que, para diferentes subconjuntos de datos de nuestra muestra calculamos el error al regularizar con ambos valores de $\lambda$ y al no regularizar. Debemos quedarnos con el modelo que menos error presente.

```{r, echo=F,warning=F}
  #library(glmnet)
  #El método para llamar a GLM con lasso (elasticnet regularization) es glmnet

  #En la documentación aqui parece que haya más families.

  #Mediante validación cruzada sacamos el mejor lambda

  testRegularization <- function(data, label, iter = 100){
      error1 <- 0
      error2 <- 0
      error3 <- 0
      
      for(i in 1:iter){
          train <- sample(nrow(data),0.7*nrow(data))
          data.train <- data[train,]
          data.test <- data[-train,]
          #Etiquetas
          ldata.train <- label[train]
          ldata.test <- label[-train]
          
          ml_lasso <- cv.glmnet(as.matrix(data.train), ldata.train, family="poisson")
          ml = ml.gaussian2 <- glm(chd ~ ., family = gaussian(log), data = data, subset=train, start=rep(0, ncol(data)+1))
    
          error1 <- error1 + calculateEtest(ml_lasso,data.test,ldata.test,ml_lasso$lambda.min)[[1]]
          error2 <- error2 +calculateEtest(ml_lasso,data.test,ldata.test,ml_lasso$lambda.1se)[[1]]
          error3 <- error3 +calculateEtest(ml,data.test,ldata.test)[[1]]
      }
    
      return(c(error1/iter,error2/iter,error3/iter))
  }

  l <- testRegularization(sahd.data, sahd.label, 100)
  
  cat("Etest con lambda min: ",l[1],"\n")
  cat("Etest con lambda 1se: ",l[2],"\n")
  cat("Etest con el modelo original: ",l[3],"\n")

```

Como podemos observar, el error medio obtenido es menor con el modelo sin regularizar, obteniendo así una respuesta negativa a la pregunta. Por tanto, seguiremos con el modelo sin regularización.

A modo de ampliación, comentar que el método de regularización lasso, cuando devuelve los coeficientes nulos significa que está despreciando estos atributos para la predicción. Si imprimimos los coeficientes correspondientes al valor de *lambda.1se* observamos que los atriburos que no selecciona para la predicción son: sbp, adiposity, typea, obesity y alcohol.

```{r, echo=F,warning=F}
  ml_lasso <- cv.glmnet(as.matrix(sahd.train), lsahd.train, family="poisson")
  coeffs.1se <- coef(ml_lasso, s = ml_lasso$lambda.1se)
  print(coeffs.1se)
```

A continuación, en la selección del número de atributos a utilizar, veremos que dichos atributos son, en efecto, algunos de los que participan en menos combinaciones "óptimas", por lo tanto, serán algunos de los menos relevantes.

## Optimización del número de atributos para el modelo seleccionado

Para optimizar el número de atributos para el modelo seleccionado, vamos a hacer uso de la función llamada `regsubsets`, esta función junto con las opciones de `method = "exhaustive"` y `nbest = 1` realizará una búsqueda exhaustiva del mejor atributo (el que produce menor error cuadrático), la mejor pareja de atributos, el mejor trío, etcétera. 

El método nos proporciona el siguiente esquema en el que podemos apreciar las combinaciones de atributos elegidos.

```{r, echo=F,warning=F}

  #method = "exhaustive" para que no sea greedy.
  testing <- regsubsets(chd ~ ., data = sahd.data, nbest = 1, method = "exhaustive")

  #Obtenemos una tablita con los que es mejor coger.
  stesting <- summary(testing)
  print(stesting$outmat)
```

En este punto nos planteamos cuántos atributos utilizar para nuestro modelo. Claramente, cuantos más atributos utilicemos menor será el error producido en la muestra. Esto lo podemos corroborar dibujando la gráfica del error por mínimos cuadrados en función del número de atributos elegidos.

```{r, echo=F,warning=F}

  #Dibujar -> testing$ress
  plot(testing$ress, col = "red", pch = 19, type = "b")
```

Sin embargo, debemos de plantearnos qué número de atributos sería el óptimo si penalizáramos también el número de atributos utilizados. Es decir, cuándo una función que combine el error producido junto con el número de atributos utilizado se minimiza. Para ello utilizamos la función *BIC*, cuya gráfica en función del número de atributos podemos observar a continuación: 

```{r, echo=F,warning=F}

  plot(stesting$bic, col = "blue", pch = 19, type = "b")
```
  
  El criterio BIC (Bayesian Information Criterion) se basa en seleccionar el modelo con menor función BIC asociada. La función BIC es una función que depende logarítmicamente del tamaño de la muestra $n$, el número de atributos $k$ y el estimador máximo verosímil, $\hat{L}$. Concretamente, $BIC = \ln(n)k-2\ln(\hat{L})$ Dicha función alcanza un mínimo con 5 atributos, por tanto, este será el número de atributos que utilizaremos para nuestro modelo.
  
```{r, echo=F,warning=F}

  sahd.data <- sahd.data[,c(2,3,5,6,9)]
```

## Transformación de atributos

A continuación, nos planteamos la búsqueda de una transformación polinómica de los atributos para la cual nuestro modelo tenga una mayor capacidad de aprender y predecir nuestro conjunto de datos. Para ello utilizamos un algoritmo de búsqueda greedy. Para cada atributo, vamos eligiendo distintos exponentes y nos quedamos con el exponente que proporcione menor error de validación. Cuando llegamos al siguiente atributo, aplicamos el mismo procedimiento, manteniendo para los atributos anteriores el mejor exponente encontrado. Además, como cuando dos modelos proporcionan resultados similares siempre es mejor quedarse con el más simple, fijaremos una tolerancia para la cual, si no hay mejoras significativas en la nueva transformación, nos quedemos con la transformación mejor obtenida previamente, que tendrá un exponente menor y por tanto será más simple el ajuste.

```{r, echo=F,warning=F}
testPolyTransform <- function(data,label){
  #Muestra
  train <- sample(nrow(data),0.7*nrow(data))
  
  #Datos
  data.train <- data[train,]
  data.test <-  data[-train,]
  
  #Etiquetas
  label.train <- label[train]
  label.test <- label[-train]
  
  #Usamos el modelo ganador
  ml <- glm(chd ~., family = gaussian(log), data = data, subset =train,start=rep(0, ncol(data)+1))
  
  models <- list(ml)
  
  testFamilies(models, data.test, label.train, label.test)
  
}

testPolyTransformRep <- function(rep,data,label){
   l <- replicate(n = rep, expr = testPolyTransform(data,label))
  #l es una matriz 3D de rep x num_modelos x 2 (Ein,Eout)
  #l[,,i] -> experimento i-ésimo
  #[,i,] -> Ein / Eout de cada modelo en los distintos experimentos (i=1 Ein, i=2 Eout)
  #[i,,] -> Ein y Eout para el modelo i-ésimo en cada experimento
  apply(FUN = mean, X = l, MARGIN = c(1,2))
}

# Devuelve un vector de coeficientes para cada atributo
# tol: Nivel de tolerancia: si dos Eout se diferencian en menos de Eout, escogemos el coeficiente más simple
polynomialTransformGreedy <- function(data,label,max_coeff, tol = 0){
  transform <- data
  coeffs <- c()

  for(i in 1:ncol(transform)){
    bestEout <- 1
    bestInd <- 0
    for(j in 1:max_coeff){
      transform[,i] <- I(data[,i]^j)
      v <- testPolyTransformRep(100,transform,label)
      # Comparamos con Etest
      if(v[2] + tol < bestEout){
        bestEout <- v[2]
        bestInd <- j
      }
      cat("Attr = ",i,", Exp = ",j,", ","Ein = ", v[1], "Eout = ",v[2],"\n")
    }
    #Nos quedamos con el mejor coeficiente obtenido para el atributo
    transform[,i] <- I(data[,i]^bestInd)
    coeffs <- c(coeffs,bestInd)
  }
  # Por ser greedy con el mecanismo escogido el mejor Eout va a ser el de la última iteración
  list(coeffs,bestEout)
}
```

Aplicamos el algoritmo. Los resultados obtenidos son:

```{r, echo=F,warning=F}
l <- polynomialTransformGreedy(sahd.data,sahd.label,6, tol = 0.01)
cat("Vector de exponentes: ", l[[1]])
cat("Eout estimado: ",l[[2]])

```

Vemos que, para exponentes de hasta tamaño 6 no se aprecian mejoras significativas con respecto a los coeficientes lineales iniciales.

## Conclusiones.

