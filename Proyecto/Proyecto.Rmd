?---
title: "PROYECTO FINAL: AJUSTE DE MODELOS NO-LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
toc: yes
---


\clearpage

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")
    library("DMwR")
    library("neuralnet")
    library("nnet")
    library("randomForest")
    library("gbm")
```


## Comprensión del problema a resolver.


```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #LECTURA DE LOS DATOS
    har.train <- read.table("./UCIHARDataset/train/X_train.txt", sep="", head = F)
    lhar.train <- read.table("./UCIHARDataset/train/y_train.txt", sep="", head = F)
    har.data.train <- cbind(lhar.train, har.train)
    
    har.test <- read.table("./UCIHARDataset/test/X_test.txt", sep="", head = F)
    lhar.test <- read.table("./UCIHARDataset/test/y_test.txt", sep="", head = F)
    har.data.test <- cbind(lhar.test, har.test)


    summary(har.train[,1:10])
```

## Preprocesado de datos.
- Los datos están normalizados y escalados [-1,1]
- Para los métodos que utilizan métricas tenemos que quitar asimetrías.

### Modificación de los atributos cualitativos.
- No los haylos.


### Tratamiento de la asimetría con BoxCox.
<!--
Como ya hemos comentado anteriormente, hay varios atributos que presentan una alta asimetría, lo cual podría hacer que los métodos de predicción que apliquemos a continuación obtengan resultados peores. Para solucionar esto, utilizaremos un método llamado BoxCox. Este método se basa en la transformación potencial según un valor ($\lambda$) para aumentar la correlación entre las variables. Para elegir la mejor potencia (mejor $\lambda$), se busca entre los $\lambda$ que proporcionen un menor error residual. Aunque en la práctica, esto se realizará de manera automátcica con la función *preProcess()* vamos a ver cómo funciona en el caso de un atributo. 
Para que se vea mejor el funcionamiento, vamos a elegir el atributo que presente una mayor asimetría. Para ello, ordenamos los atributos en función de su asimetría:
-->

- Hay que quitar asimetría.

```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    har_asymmetry <- apply(har.train, 2, skewness)
    har_asymmetry <- sort(abs(har_asymmetry), decreasing = T)
    print(head(har_asymmetry))
    
```


### Llamada al método PreProcess.
<!--
Una vez entendidas las modificaciones que vamos a realizar a los datos, utilizaremos el método preProcess que se encarga de realizar todas estas modificaciones sobre el conjunto de datos pasado como argumento. Como ya hemos comentado, no vamos a aplicar el método *PCA*, luego la llamada quedaría de la siguiente forma:
-->

Llamamos al método PreProcess con Boxcox: quitar asimetría, PCA: para eliminar atributos y center y scale pues aunque los datos ya estaban centrados y escalados los otros métodos afectan a estas propiedades.

```{r, echo=T, warning=F}

ObjetoTrans <- preProcess(har.train,method = c("pca", "BoxCox", "center", "scale"), thres = 0.7)
#0.95 -> 102 componentes
#0.9 -> 63 componentes
#0.85 -> 40 componentes
#0.8 -> 26 componentes
#0.75 -> 16 componentes
#0.7 -> 10 componentes

har.trans.train <- predict(ObjetoTrans, har.train)
har.trans.test <- predict(ObjetoTrans, har.test)

```

## Conjuntos de validación, training y test usados.

A continuación pasaremos a explorar los distintos modelos sobre los que resolver el problema de clasificación para nuestro conjunto de datos. El procedimiento de validación que usaremos consistirá en tomar múltiples veces distintos conjuntos de entrenamiento sobre nuestro conjunto de datos (una vez transformados), con los que aprenderemos el modelo. Usaremos el resto del conjunto como test para evaluar cómo de bien predice el modelo aprendido con nuevos datos. Las proporciones utilizadas serán del 70 % de los datos para train, y el 30 % para test.

```{r, echo=F, warning=F}
  #Paso 5: Preparación de conjunto de train/test, y de las etiquetas train/test
  har.train <- har.trans.train  
  har.test <- har.trans.test
  
```

#Modelo lineal.


```{r, echo=F,warning=F}
# Funciones para el cálculo del error
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

# Función para evaluar loserrores para un conjunto de modelos.
# Argumentos:
# models - lista de modelos
# test - conjunto de test
# ltrain - etiquetas de train
# ltest - etiquetas de test
# Devuelve: Matriz de num_modelos x 2. Las columnas representan (Ein, Etest)
testFamilies <- function(models, test, ltrain, ltest){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```


## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por la función `glm` (Generalized Linear Models) de `R`. Para cada familia, siempre que admitan, utilizaremos distintas funciones de enlace. Las funciones de enlace nos permiten establecer una relación entre la media de la respuesta y los predictores del modelo. Las familias que vamos a considerar para el ajuste son:

- **Binomial**, con link **logit**. Regresión logística.
- **Binomial**, con link **probit**. Modelo binomial, con función de enlace $\Phi^{-1}(\mu)$, donde $\Phi$ es la distribución acumulada de la distribución normal.
- **Binomial**, con link **cauchit**. Modelo binomial, cuya función de enlace es la análoga a la del modelo anterior sobre una distribución de Cauhy, en lugar de la normal.
- **Gaussiana**, con link **identity**. Regresión lineal.
- **Gaussiana**, con link **log**. Distribución normal con función de enlace logarítmica.
- **Poisson**. Distribución de Poisson. 
- **Quasi**. Este modelo no tiene una varianza determinada como en el resto de familias. Indicaremos la especificación de varianza `"constant"`
- **Quasibinomial.** Distribución binomial, con la única diferencia de que no fija el parámetro de dispersión (intenta describir varianza adicional en los datos que no puede ser explicada mediant una distribución binomial).
- **Quasipoisson.** Distribución de Poisson, con la única diferencia de que no fija el parámetro de dispersion.



```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

testFamilies <- function(models, test, ltrain, ltest, modelNames = NULL){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  if(!is.null(modelNames)){
    rownames(results) <- modelNames
  }
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```

```{r, echo=F,warning=F}
  balance <- function(train, test, ltrain, ltest){
    #Juntamos datos con etiquetas
    train.aux <- cbind(train, ltrain)
    test.aux <- cbind(test, ltest)
    
    
    train.aux$ltrain <- as.factor(train.aux$ltrain)
    test.aux$ltest <- as.factor(test.aux$ltest)
    
    #Balanceamos
    train.aux <- SMOTE(ltrain~., train.aux, perc.over = 100, perc.under = 200)
    test.aux <- SMOTE(ltest~., test.aux, perc.over = 100, perc.under = 200)
    
    #Volvemos a separar datos de etiquetas
    train.aux$ltrain <- as.numeric(train.aux$ltrain)
    test.aux$ltest <- as.numeric(test.aux$ltest)
  
    
    train <- train.aux[,-ncol(train.aux)]
    ltrain <- train.aux[,ncol(train.aux)] - 1
  
    
    test <- test.aux[,-ncol(test.aux)]
    ltest <- test.aux[,ncol(test.aux)] - 1
    
    return(list(train,test,ltrain,ltest))
  }
```

```{r, echo=F,warning=F}
validateLinearModels <- function(train,test,ltrain,ltest){
  count_1 = sum(ltrain==1)
  count_0 = sum(ltrain==0)
  count = count_1 + count_0
  
  if(count_1/count <= 0.1 || count_0/count <= 0.1){
      l <- balance(train, test, ltrain, ltest)
      train <- l[[1]]
      test <- l[[2]]
      ltrain <- l[[3]]
      ltest <- l[[4]]
  }
  
  #Modelos
  #binomial
  ml.binomial1 <- glm(ltrain ~ ., family = binomial(logit), data = train)
  ml.binomial2 <- glm(ltrain ~ ., family = binomial(probit), data = train)
  ml.binomial3 <- glm(ltrain ~ ., family = binomial(cauchit), data = train)
  #gaussiano
  ml.gaussian1 <- glm(ltrain ~ ., family = gaussian(identity), data = train)
  ml.gaussian2 <- glm(ltrain ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
  #poisson
  ml.poisson1 <- glm(ltrain ~ ., family = poisson(log), data = train)
  
  #quasi
  ml.quasi1 <- glm(ltrain ~ ., family = quasi(link = "identity", variance = "constant"), data = train)

  #quasibinomial
  ml.quasibinomial1 <- glm(ltrain ~ ., family = quasibinomial(link = "logit"), data = train)
  #quasipoisson
  ml.quasipoisson1 <- glm(ltrain ~ ., family = quasipoisson(link = "log"), data = train)  
  models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
    #modelNames <- c("Gaussian","Poisson")
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  testFamilies(models, test, ltrain, ltest, modelNames)
}


```


Una vez definidos los modelos y las funciones a usar, procedemos al ajuste de los distintos modelos y al análisis de sus errores:

```{r, echo=F,warning=F}
validateLinearAllLabels <- function(train,test,ltrain,ltest){
    output <- matrix(nrow = 9, ncol = 12)
    rownames(output) <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
    colnames(output) <- c("Ein 1", "Eout 1","Ein 2", "Eout 2","Ein 3", "Eout 3","Ein 4", "Eout 4","Ein 5", "Eout 5","Ein 6", "Eout 6")
    for( i in 1:6){
       cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       M <- validateLinearModels(train,test,ltrain_i,ltest_i)
       
       output[,c(2*i-1,2*i)] <- M[,c(1,2)]
    } 
    output
}
  
```

```{r, echo=F,warning=F}
validateLinearAllLabels(har.train,har.test,as.vector(lhar.train[,1]),as.vector(lhar.test[,1]))

```

#Redes Neuronales

En primer lugar aplicamos validación cruzada para optimizar:
- size: número de unidades ocultas intermedias
- decay: se usa para evitar el sobre ajuste

Como queremos usarlo para clasificación debemos usar el parámetro: lineout = T.

```{r, echo=F,warning=F}
  #train.data <- cbind(har.train, lhar.train)
```

Lo hacemos hasta 25 porque peta. La función tune obtiene por validación cruzada el error para un tamaño de capa oculta prefijada.

```{r, echo=F,warning=F}
  tuneNNet <- function(data, label){
    #Probamos los parámetros con tune.nnet
    for(i in 1:30){
        s[i] <- (tune.nnet(x = data, y = label, size = i))
    }
    
    s <- cbind(c(1:30), s)
    s
  }
  
```

```{r, echo=F,warning=F}
  tuneNNet(har.train, lhar.train)

tune_nnet <- tune(nnet,train.x = har.train, train.y = as.factor(lhar.train[,1]), ranges = list(size = 1:50))
```

El número de unidades óptimo para una sola capa oculta es: 19. 

Probamos ahora entre las redes neuronales de:
  - Una capa con 19 unidades
  - Dos capas con 19 unidades cada una.
  - Tres capas con 19 unidades cada una.
  
  Vamos a utilizar la validación cruzada de mhe
  
   = data.aux[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
  
```{r, echo=F,warning=F}

  chooseNumberLayers <- function(data, label, size){
    #data.aux<-cbind(data,label)
    #linear.output -> True para clasificacion
    #algorithm -> por defecto rprop+ (?)

    per <- sample(nrow(data), nrow(data))
    #Barajamos
    data[per,]

    #Calculamos error con una capa
    m <- matrix(nrow = 3, ncol = 1)
  
    for(j in 1:3){
       error <- 0
       for(i in 1:5){
          #Esto cogido de internet -> CUIDADO
          train <- data[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
          ltrain <- label[((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5),]
          n <- names(train)
          f <- as.formula(paste("ltrain ~", paste(n[!n %in% "ltrain"], collapse = " + ")))
          m <- neuralnet(f, train , hidden = rep(size,j), linear.output = TRUE)
          
          #No estoy segura de como calcular el error 
          ltest <- label[-(((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5)),]
          test <- data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),]
          pnn.m <- compute(m, data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),])  
          pr.nn_ <- pnn.m$net.result*(max(ltrain)-min(ltrain))+min(ltrain)
          test.r <- (ltest)*(max(ltrain)-min(ltrain))+min(ltrain)

          error < error  + sum((test.r - pr.nn_)^2)/nrow(test)
       }   
       
       m[j] <- error/5
    }

    
  }

```


```{r, echo=F,warning=F}
  chooseNumberLayers(har.train, lhar.train, 19)
```

#Máquina de Soporte de Vectores

- Los datos están normalizados.

```{r, echo=F,warning=F}

tc <- tune.control(cross = 5)

#svm_model <- svm(x = har.train, y = factor(lhar.train[,1]), kernel = "radial",gamma = 0.1 )
svm_tune <- tune(svm, train.x = har.train, train.y = factor(lhar.train[,1]), kernel = "radial", ranges = list(gamma=c(0.01,0.1,1,10)), tunecontrol = tc)
```


#Boosting

```{r, echo=F,warning=F}

  gbm_algorithm <- gbm(y ~ ., data = train_dataset, distribution = "adaboost", n.trees = 5000)

```


#RandomForest



Hay dos parámetros importantes para predecir:
1- Número de árboles (ntree)
2- Número de variables aleatorias usadas en cada árbol (mtry)

Calculamos el número de variables aleatorias usadas en cada árbol óptimo:

```{r, echo=F,warning=F}
    #La función tuneRF calcula a partir del valor por defecto de mtry el valor óptimo de mtry para el randomForest
    #Convertimos la etiqueta a un factor para que haga clasificación
    flhar.train <- as.factor(lhar.train[,1])
    best.mtry <- tuneRF(har.train, flhar.train, stepFactor = 1, improve = 0.02, ntree = 500)
```

-> Es sqrt(9) que es el valor optimo en teoria.


Calculamos el número de árboles óptimo:

```{r, echo=F,warning=F}
   best.params <- tune(method = randomForest, har.train, flhar.train, ranges = list(ntree = c(10,20,30), mtry = 3))

```

