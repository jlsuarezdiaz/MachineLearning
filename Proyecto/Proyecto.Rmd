---
title: "PROYECTO FINAL: AJUSTE DE MODELOS NO-LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
toc: yes
---

<!--
  6- Por qué SVM tarda mucho -> Porque los datos están muy juntos -- lo de espradao y etar andando
  7- boosting -> cuidad con el ruido -> regularizar
  8- En neuronas: Tasa de aprendizaje, mini/max, funcion de error (gradiente estocastic o no)
-->
\clearpage

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")
    library("DMwR")
    library("neuralnet")
    library("nnet")
    library("randomForest")
    library("gbm")
    library("adabag")
    library("maboost")
```


## Comprensión del problema a resolver: Human Activity Recognition Using Smartphones Data Set.

La base de datos elegida para el problema recoge la información de un grupo 30 voluntarios con edades comprendidas entre 18-48 años. Cada uno de estos individuos tenía que realizar una serie de actividades con el teléfono móvil enganchado en su cintura. Así, utilizando el dispositivo se podían registrar la aceleración lineal y angular en 3-ejes. Los experimentos se grabana en vídeo para poder etiquetar los datos de forma manual. De entre los datos recogidos, se seleccionaron de forma aleatoria el 70\% de esto para hacer de conjunto de training mientras que el otro 30\% se encargaba de recoger los datos para el test. 
Así, la base de datos se compone de 7352 muestras en el conjunto de train y 2947 en el conjunto de test, formando un total de 10299 muestras recogidas de estos 30 voluntarios y se consideran un total de 561 atributos.

Observamos el resumen de los diez primeros atributos:

```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #LECTURA DE LOS DATOS
    har.train <- read.table("./UCIHARDataset/train/X_train.txt", sep="", head = F)
    lhar.train <- read.table("./UCIHARDataset/train/y_train.txt", sep="", head = F)
    har.data.train <- cbind(lhar.train, har.train)
    
    har.test <- read.table("./UCIHARDataset/test/X_test.txt", sep="", head = F)
    lhar.test <- read.table("./UCIHARDataset/test/y_test.txt", sep="", head = F)
    har.data.test <- cbind(lhar.test, har.test)


    print(summary(har.train[,1:10]))
```

En cuanto a las etiquetas, nos encontramos ante un problema de clasificación multiclase, dado que las etiquetas toman valores enteros en el intervalo [1,6]. Cada etiqueta clasifica el movimiento registrado por el dispositivo móvil y lo clasifica en seis tipos:
  1. WALKING
  2. WALKING_UPSTAIRS
  3. WALKING_DOWNSTAIRS
  4. SITTING
  5. STANDING
  6. LAYING
  
Podemos observar fijándonos en lo que representa cada etiqueta que los tres primeros valores de las etiquetas hacen referencia a actividades muy similares, relacionadas con andar, mientras que las otras tres tienen la misma propiedad, representando estados de no movimiento. Podemos intuir por tanto, en vista a lo que representan las distintas clases, que los datos de las tres primeras clases serán fácilmente separables de los datos de las tres restantes, si las medidas que se han tomado son representativas del movimiento, mientras que a priori será más complicado distinguir dentro de cada grupo de clases, cuál será la asignación correcta.

Al tratarse de un problema de clasificación multiclase, para la aplicación de aquellos métodos que precisen de clasificación binaria para un buen funcionamiento utilizaremos la técnica de *One vs One*, la cual consistirá en dividir nuestro problema de clasificación multiclase en $\binom{6}{2} = 15$ problemas de clasificación binaria de la forma clase_i vs clase_j con $i,j \in [1,6]$,

Así, el cálculo del error para cada modelo propuesto consistirá en el número de errores cometidos. Esto es, número de muestras con etiqueta asignada de forma errónea. Para aquellos modelos que precisen de una adaptación a subproblemas de clasificación binaria, obtendremos quince vectores con la probabilidad de asignación de cada una de las seis etiquetas disponibles. Por tanto, le asignaremos a cada elemento la etiqueta que obtenga una mayor probabilidad, calculando el error en este caso de la misma forma que para el resto de modelos.

## Preprocesado de datos.

  En primer lugar, observamos que nuestra base de datos no contiene valores perdidos (NaNs), por lo cual no será necesario el trato especial de estos valores. Lo mismo ocurre con las variables cualitativas.
  
  En cuanto al desbalanceo, observemos el porcentaje de elementos de cada clase en el train y después en el test:

```{r, echo=F,warning=F}
  #Estudiamos si hay desbalanceo
  print("El porcentaje de elementos con etiqueta 1 en el train es: ")
  print(100*sum(lhar.train == 1)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 2 en el train es: ")
  print(100*sum(lhar.train == 2)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 3 en el train es: ")
  print(100*sum(lhar.train == 3)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 4 en el train es: ")
  print(100*sum(lhar.train == 4)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 5 en el train es: ")
  print(100*sum(lhar.train == 5)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 6 en el train es: ")
  print(100*sum(lhar.train == 6)/nrow(lhar.train))
```


```{r, echo=F,warning=F}

  print("El porcentaje de elementos con etiqueta 1 en el test es: ")
  print(100*sum(lhar.test == 1)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 2 en el test es: ")
  print(100*sum(lhar.test == 2)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 3 en el test es: ")
  print(100*sum(lhar.test == 3)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 4 en el test es: ")
  print(100*sum(lhar.test == 4)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 5 en el test es: ")
  print(100*sum(lhar.test == 5)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 6 en el test es: ")
  print(100*sum(lhar.test == 6)/nrow(lhar.test))
  
  
    
```  
  
  Podemos observar que no existe desbalance entre las seis clases contempladas, luego no podemos despreciar ninguna. 
  
  Aunque los datos están ya normalizados y escalados en [-1,1], estos presentan una gran asimetría como podemos observar:
  
```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    har_asymmetry <- apply(har.train, 2, skewness)
    har_asymmetry <- sort(abs(har_asymmetry), decreasing = T)
    print(head(har_asymmetry))
    
```
  
  Si representamos el atributo con mayor asimetría de todos obtenemos:

```{r, echo=F,warning=F}
  hist(har.train$V389, col = "blue")
```  

  Aunque los datos presentan una gran asimetría, no los quitamos porque "rompen el esquema".
  
  Además, la base de datos presenta un número demasiado alto de atributos, lo cual puede ser un gran problema a la hora utilizar métodos costosos para la predicción de las etiquetas. Con el objetivo de reducir el número de atributos sin perder información relevante, vamos a probar con dos métodos:
  
##Analisis de componentes principales (PCA)
  
  En primer lugar, probaremos a aplicar PCA a los datos con el objetivo de encontrar dependencias entre las variables y simplificar el conjunto de datos reduciendo los tiempos de cómputo. Los nuevos atributos considerados tras aplicar este método son combinaciones lineales de los atributos originales que recogen un tanto por ciento de la varianza acumulada de los datos pasado como argumento (*thresh*). Para la aplicación vamos a considerar *thresh = 0.95* ...
  
  Utilizaremos la función *preProcess* pasándole como argumento *method = c(pca)*.


```{r, echo=T, warning=F}
ObjetoTrans <- preProcess(har.train,method = c("pca"), thresh = 0.95)
```

```{r, echo=T, warning=F}
  print(ObjetoTrans)
```

Para mantener el 95\% de la varianza se necesitan mantener 102 atributos.

##Regresión LASSO (least absolute shrinkage and selection operator).

  Lasso es un método que lleva a cabo la regularización a la misma vez que realiza selección de características. 

El objetivo se basa en reducir el error de predicción, para ello, se ocupa de reducir la función:

\[
  R(\beta) = \sum_{i=1}^{n}(y_i - x_i\omega)^2 + \lambda \sum_{i=1}^{p}|\omega_i|
\]

donde $n$ es el número de muestras, $p$ el número de atributos y $w$ el vector de pesos solución. Así, obtiene diferentes valores de $\lambda$. Entre estos valores devueltos, podemos considerar dos de ellos:

- *lambda.min*, que nos devuelve el valor del $\lambda$ para el que se minimiza el error obtenido.
- *lambda.1se*. Aunque el $\lambda$ anterior sea menor, presenta una mayor dependencia de las particiones seleccionadas en la validación cruzada. La regla `1se` (one standard error), elige un valor de $\lambda$ para lo suficiente cercano a `lambda.min` para el cual el modelo obtenido sea lo suficiente simple, reduciendo así la dependencia del error respecto a las validaciones.

Podemos contemplar cualquiera de estos valores de $\lambda$ para regularizar nuestro modelo. 

Para obtener el modelo LASSO, realizaremos validación cruzada con tres particiones (dada la elevada dimensión de la base de datos). Tras obtener el modelo, considerando cada uno de los valores de *lambda* anteriormente comentado, obtenemos seis vectores de coeficientes donde cada uno de las componentes de estos vectores representa la relevancia de cada atributo para cada clase. Así, los atributos de los cuales podremos prescindir serán aquellos en los que la componente correspondiente a dicho atributo en todos los vectores sea nula. Realizando estas simplificaciones obtenemos que tras simplificar atributos las dimensiones obtenidas son:

```{r, echo=T, warning=F}
    ml_lasso <- cv.glmnet(as.matrix(har.train), lhar.train[,1], family="multinomial", nfolds = 3)
    lambda.min <- ml_lasso$lambda.min
    lambda.1se <- ml_lasso$lambda.1se
    
    coeffs.min <- coef(ml_lasso, s = ml_lasso$lambda.min)
    coeffs.1se <- coef(ml_lasso, s = ml_lasso$lambda.1se)
    
    print(coeffs.min)
    print(coeffs.1se)
    
    #Eliminamos los atributos del train  

    coefficients.min <- matrix(ncol = 6, nrow = 562)
    coefficients.1se <- matrix(ncol = 6, nrow = 562)
    for(i in 1:6){
      coefficients.min[,i] <- (coeffs.min[[i]])[1:nrow(coeffs.min[[i]]),]
      coefficients.1se[,i] <- (coeffs.1se[[i]])[1:nrow(coeffs.1se[[i]]),]
    }
```
 
 
   
```{r, echo=T, warning=F}
    
    har.train_lasso.min = har.train[,abs(coefficients.min[1:6,])>0]
    har.train_lasso.1se = har.train[,abs(coefficients.1se[1:6,])>0]
    
    print("El número de atributos tras aplicar la reducción con lambda.min es: ")
    print(dim(har.train_lasso.min)[2])
    
    print("El número de atributos tras aplicar la reducción con lambda.1se es: ")
    print(dim(har.train_lasso.1se)[2])

```

Por tanto, aunque sería preferible aplicar una reducción de atributos basada en la regresión LASSO dado que los atributos obtenidos se corresponden con los atributos originales del modelo, obtenemos que tras comparar las tres reducciones de dimensionalidad la que mayor reducción produce es la aplicación de *PCA*. En conclusión, a partir de ahora trabajaremos con el conjunto de datos resultante de aplicar la reducción con *PCA*.


```{r, echo=T, warning=F}
har.trans.train <- predict(ObjetoTrans, har.train)
har.trans.test <- predict(ObjetoTrans, har.test)

har.train <- har.trans.train  
har.test <- har.trans.test
```

#Modelo lineal.

## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por la función `glm` (Generalized Linear Models) de `R`. Para cada familia, siempre que admitan, utilizaremos distintas funciones de enlace. Las funciones de enlace nos permiten establecer una relación entre la media de la respuesta y los predictores del modelo. Las familias que vamos a considerar para el ajuste son:

- **Binomial**, con link **logit**. Regresión logística.
- **Binomial**, con link **probit**. Modelo binomial, con función de enlace $\Phi^{-1}(\mu)$, donde $\Phi$ es la distribución acumulada de la distribución normal.
- **Binomial**, con link **cauchit**. Modelo binomial, cuya función de enlace es la análoga a la del modelo anterior sobre una distribución de Cauhy, en lugar de la normal.
- **Gaussiana**, con link **identity**. Regresión lineal.
- **Gaussiana**, con link **log**. Distribución normal con función de enlace logarítmica.
- **Poisson**. Distribución de Poisson. 
- **Quasi**. Este modelo no tiene una varianza determinada como en el resto de familias. Indicaremos la especificación de varianza `"constant"`
- **Quasibinomial.** Distribución binomial, con la única diferencia de que no fija el parámetro de dispersión (intenta describir varianza adicional en los datos que no puede ser explicada mediant una distribución binomial).
- **Quasipoisson.** Distribución de Poisson, con la única diferencia de que no fija el parámetro de dispersion.



```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.prob_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.prob_train))
}

testFamilies <- function(models, test, ltrain, ltest, modelNames = NULL){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  if(!is.null(modelNames)){
    rownames(results) <- modelNames
  }
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```

```{r, echo=F,warning=F}
  balance <- function(train, test, ltrain, ltest){
    #Juntamos datos con etiquetas
    train.aux <- cbind(train, ltrain)
    test.aux <- cbind(test, ltest)
    
    
    train.aux$ltrain <- as.factor(train.aux$ltrain)
    test.aux$ltest <- as.factor(test.aux$ltest)
    
    #Balanceamos
    train.aux <- SMOTE(ltrain~., train.aux, perc.over = 100, perc.under = 200)
    test.aux <- SMOTE(ltest~., test.aux, perc.over = 100, perc.under = 200)
    
    #Volvemos a separar datos de etiquetas
    train.aux$ltrain <- as.numeric(train.aux$ltrain)
    test.aux$ltest <- as.numeric(test.aux$ltest)
  
    
    train <- train.aux[,-ncol(train.aux)]
    ltrain <- train.aux[,ncol(train.aux)] - 1
  
    
    test <- test.aux[,-ncol(test.aux)]
    ltest <- test.aux[,ncol(test.aux)] - 1
    
    return(list(train,test,ltrain,ltest))
  }
```

```{r, echo=F,warning=F}
predictions <- function(train, test, ltrain, ltest, modelNames = NULL){
  
  M <- matrix(nrow = 9, ncol = length(ltest))
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial1 <- glm(ltrain_i ~ ., family = binomial(logit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[1,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial2 <- glm(ltrain_i ~ ., family = binomial(probit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial2,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[2,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial3 <- glm(ltrain_i ~ ., family = binomial(cauchit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial3,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[3,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.gaussian1 <- glm(ltrain_i ~ ., family = gaussian(identity), data = train)
       
       preds_j[i,] <- calculateEtest(ml.gaussian1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[4,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.gaussian2 <- glm(ltrain_i ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
       
       preds_j[i,] <- calculateEtest(ml.gaussian2,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[5,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.poisson1 <- glm(ltrain_i ~ ., family = poisson(log), data = train)
       
       preds_j[i,] <- calculateEtest(ml.poisson1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[6,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.quasi1 <- glm(ltrain_i ~ ., family = quasi(link = "identity", variance = "constant"), data = train)
       
       preds_j[i,] <- calculateEtest(ml.quasi1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[7,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
        ml.quasibinomial1 <- glm(ltrain_i ~ ., family = quasibinomial(link = "logit"), data = train)
       
       preds_j[i,] <- calculateEtest(ml.quasibinomial1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[8,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.quasipoisson1 <- glm(ltrain_i ~ ., family = quasipoisson(link = "log"), data = train) 
       
       preds_j[i,] <- calculateEtest(ml.quasipoisson1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[9,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  
  
  
  
 
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  rownames(M) <- modelNames
  
  errorVec <- vector(length = 9)
  for(i in 1:9){
      errorVec[i] <- sum(M[i,]!=ltest)/length(ltest)
  }
  names(errorVec) <- modelNames
    
  return(list(M,errorVec))  
}

```

```{r, echo=F,warning=F}
P <- predictions(har.train,har.test,lhar.train[,1],lhar.test[,1])

# Matriz de confusión para Poisson (que proporciona el menor error)
Eout.linear <- P[[2]][6]
confussion.linear <- table(P[[1]][6,],lhar.test[,1])


```

```{r, echo=F,warning=F}
validateLinearModels <- function(train,test,ltrain,ltest){
  count_1 = sum(ltrain==1)
  count_0 = sum(ltrain==0)
  count = count_1 + count_0
  
  if(count_1/count <= 0.1 || count_0/count <= 0.1){
      l <- balance(train, test, ltrain, ltest)
      train <- l[[1]]
      test <- l[[2]]
      ltrain <- l[[3]]
      ltest <- l[[4]]
  }
  
  #Modelos
  #binomial
  ml.binomial1 <- glm(ltrain ~ ., family = binomial(logit), data = train)
  ml.binomial2 <- glm(ltrain ~ ., family = binomial(probit), data = train)
  ml.binomial3 <- glm(ltrain ~ ., family = binomial(cauchit), data = train)
  #gaussiano
  ml.gaussian1 <- glm(ltrain ~ ., family = gaussian(identity), data = train)
  ml.gaussian2 <- glm(ltrain ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
  #poisson
  ml.poisson1 <- glm(ltrain ~ ., family = poisson(log), data = train)
  
  #quasi
  ml.quasi1 <- glm(ltrain ~ ., family = quasi(link = "identity", variance = "constant"), data = train)

  #quasibinomial
  ml.quasibinomial1 <- glm(ltrain ~ ., family = quasibinomial(link = "logit"), data = train)
  #quasipoisson
  ml.quasipoisson1 <- glm(ltrain ~ ., family = quasipoisson(link = "log"), data = train)  
  models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
    #modelNames <- c("Gaussian","Poisson")
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  testFamilies(models, test, ltrain, ltest, modelNames)
  
}


```




Una vez definidos los modelos y las funciones a usar, procedemos al ajuste de los distintos modelos y al análisis de sus errores:

```{r, echo=F,warning=F}
validateLinearAllLabels <- function(train,test,ltrain,ltest){
    output <- matrix(nrow = 9, ncol = 12)
    rownames(output) <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
    colnames(output) <- c("Ein 1", "Eout 1","Ein 2", "Eout 2","Ein 3", "Eout 3","Ein 4", "Eout 4","Ein 5", "Eout 5","Ein 6", "Eout 6")
    for( i in 1:6){
       #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       M <- validateLinearModels(train,test,ltrain_i,ltest_i)
       
       output[,c(2*i-1,2*i)] <- M[,c(1,2)]
    } 
    output
}
  
```

```{r, echo=F,warning=F}
validateLinearAllLabels(har.train,har.test,as.vector(lhar.train[,1]),as.vector(lhar.test[,1]))

```

#Redes Neuronales

En primer lugar aplicamos validación cruzada para optimizar:
- size: número de unidades ocultas intermedias
- decay: se usa para evitar el sobre ajuste

Como queremos usarlo para clasificación debemos usar el parámetro: lineout = T.

```{r, echo=F,warning=F}
# Controlador de tune. Debido al tamaño de los datos, utlizaremos validación cruzada 5-fold para la estimación de hiperparámetros.
tc <- tune.control(cross = 5)
```


```{r, echo=F,warning=F}
  #train.data <- cbind(har.train, lhar.train)
```

Lo hacemos hasta 25 porque peta. La función tune obtiene por validación cruzada el error para un tamaño de capa oculta prefijada.

```{r, echo=F,warning=F}
  tuneNNet <- function(data, label){
    #Probamos los parámetros con tune.nnet
    for(i in 1:30){
        s[i] <- (tune.nnet(x = data, y = label, size = i))
    }
    
    s <- cbind(c(1:30), s)
    s
  }
  
```

Transformación de etiquetas para `nnet`. Para evitar tratar con datos categóricos (etiquetas $1,\dots,6$), que no pueden ser tratados con `nnet`, creamos 6 columnas de etiquetas a 0 o 1.

```{r, echo=F,warning=F}
lhar.train.multi01 <- class.ind(lhar.train[,1])
har.data.nnet <- cbind(har.train,lhar.train.multi01)

names(har.data.nnet) <- c(names(har.data.nnet)[1:102], "l1", "l2", "l3", "l4", "l5", "l6")
n <- names(har.data.nnet)[1:102]

# Obtenemos la fórmula:
f.nnet <- as.formula(paste("l1 + l2 + l3 + l4 + l5 + l6 ~",paste(n[!n %in% c("l1","l2","l3","l4","l5","l6")], collapse = " + ")))

```

```{r, echo=F,warning=F}
  #tuneNNet(har.train, lhar.train)
tune_nnet <- tune(nnet,train.x = f.nnet, data = har.data.nnet, ranges = list(size = 1:5),tunecontrol = tc)
```

Predecimos el error sobre la muestra test para el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

nnet_model <- nnet(formula = f.nnet, data = har.data.nnet, size = 1)
summary(nnet_model)


```

```{r, echo=F,warning=F}

har.test.nnet <- har.test
names(har.test.nnet) <- c("V1","V2","V3","V4","V5","V6","V7","V8","V9","V10")
#nnet.pred <- predict(nnet_model,har.test.nnet, type = "raw")
# No sé por qué sale todo 1 aquí

#compute(nnet_model,har.test.nnet)
```

El número de unidades óptimo para una sola capa oculta es: 19. 

Probamos ahora entre las redes neuronales de:
  - Una capa con 19 unidades
  - Dos capas con 19 unidades cada una.
  - Tres capas con 19 unidades cada una.
  
  Vamos a utilizar la validación cruzada de mhe
  
   = data.aux[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
  
```{r, echo=F,warning=F}

  chooseNumberLayers <- function(data, label, size){
    #data.aux<-cbind(data,label)
    #linear.output -> True para clasificacion
    #algorithm -> por defecto rprop+ (?)

    per <- sample(nrow(data), nrow(data))
    #Barajamos
    data[per,]

    #Calculamos error con una capa
    m <- matrix(nrow = 3, ncol = 1)
  
    for(j in 1:3){
       error <- 0
       for(i in 1:5){
          #Esto cogido de internet -> CUIDADO
          train <- data[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
          ltrain <- label[((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5),]
          n <- names(train)
          f <- as.formula(paste("ltrain ~", paste(n[!n %in% "ltrain"], collapse = " + ")))
          m <- neuralnet(f, train , hidden = rep(size,j), linear.output = TRUE)
          
          #No estoy segura de como calcular el error 
          ltest <- label[-(((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5)),]
          test <- data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),]
          pnn.m <- compute(m, data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),])  
          pr.nn_ <- pnn.m$net.result*(max(ltrain)-min(ltrain))+min(ltrain)
          test.r <- (ltest)*(max(ltrain)-min(ltrain))+min(ltrain)

          error < error  + sum((test.r - pr.nn_)^2)/nrow(test)
       }   
       
       m[j] <- error/5
    }

    
  }

```


```{r, echo=F,warning=F}
  chooseNumberLayers(har.train, lhar.train, 19)
```

#Máquina de Vectores Soporte

- Los datos están normalizados.

```{r, echo=F,warning=F}

tc <- tune.control(cross = 5)

svm_tune <- tune(svm, train.x = har.train, train.y = factor(lhar.train[,1]), kernel = "radial", ranges = list(gamma=c(0.01,0.1,1,10)), tunecontrol = tc)
```

Evaluamos sobre los datos test el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

svm_model <- svm(x = har.train, y = factor(lhar.train[,1]), kernel = "radial", gamma = 0.1)
summary(svm_model)


```

```{r, echo=F,warning=F}

svm.pred <- predict(svm_model,har.test)
Eout.svm <- sum(svm.pred != lhar.test[,1])/length(svm.pred)
confussion.svm <- table(svm.pred,lhar.test[,1])
  
```


#Boosting

```{r, echo=F,warning=F}

  #gbm_algorithm <- gbm(y ~ ., data = har.train, distribution = "adaboost", n.trees = 5000)
  blhar.train <- lhar.train[,1]
  blhar.test <- lhar.test[,1]
   
  boost_mat <- matrix(nrow = 6, ncol = length(blhar.test))
  
  for(i in 1:6){
  
    ltrain_i <- blhar.train
    ltest_i <- blhar.test
         
    ltrain_i[blhar.train != i] <- 0
    ltrain_i[blhar.train == i] <- 1
    ltest_i[blhar.test != i] <- 0
    ltest_i[blhar.test == i] <- 1
    
    boost_model <- gbm(ltrain_i ~ ., data = har.train, distribution = "adaboost", n.trees = 5000)
    
    boost_mat[i,] <- predict(boost_model,har.test,n.trees = 5000,type = "response" )
  
  }
  
  boost.pred <- vector(length = length(blhar.test))
  for(i in 1:length(blhar.test)){
      boost.pred[i] = which(boost_mat[,i]==max(boost_mat[,i]))[1]
  }
  
  Eout.adagbm <- sum(boost.pred!=blhar.test)/length(blhar.test)
  confussion.adabgm <- table(boost.pred,blhar.test)
  

```



```{r, echo=T, warning=F}
#sparsefactor -> true para aplicar regularización explicita norma L1 (?)
maboost_model <- maboost(x = har.train, y = as.factor(lhar.train[,1]), sparsefactor = TRUE)  

maboost.pred <- predict(maboost_model,har.test)
Eout.maboost <- sum(maboost.pred != lhar.test[,1])/length(maboost.pred)
confussion.maboost <- table(maboot.pred,lhar.test[,1])

```

#RandomForest



Hay dos parámetros importantes para predecir:
1- Número de árboles (ntree)
2- Número de variables aleatorias usadas en cada árbol (mtry)

Calculamos el número de variables aleatorias usadas en cada árbol óptimo:

```{r, echo=F,warning=F}
    #La función tuneRF calcula a partir del valor por defecto de mtry el valor óptimo de mtry para el randomForest
    #Convertimos la etiqueta a un factor para que haga clasificación
    flhar.train <- as.factor(lhar.train[,1])
    best.mtry <- tuneRF(har.train, flhar.train, stepFactor = 1, improve = 0.02, ntree = 500)
```

-> Es sqrt(9) que es el valor optimo en teoria.


Calculamos el número de árboles óptimo:

```{r, echo=F,warning=F}
   best.params <- tune(method = randomForest, har.train, flhar.train, ranges = list(ntree = c(10,20,30), mtry = 3), tunecontrol = tc)

```

Evaluamos sobre los datos test el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

rf_model <- randomForest(x = har.train, y = flhar.train, ntree = 30, mtry = 3)
summary(rf_model)


```

```{r, echo=F,warning=F}

rf.pred <- predict(rf_model,har.test)
Eout.rf <- sum(rf.pred != lhar.test[,1])/length(rf.pred)
confussion.rf <- table(rf.pred,lhar.test[,1])
  
```

```{r, echo=F,warning=F}

model_errors <- c(Eout.linear,Eout.svm,Eout.rf,Eout.adagbm,Eout.maboost)
model_confussions <- list(confussion.linear,confussion.svm,confussion.rf,confussion.adabgm,confussion.maboost)
model_names <- c("Lineal - Poisson", "SVM", "Random Forest","Adaboost - BGM", "Adaboost - MABoost")
names(model_errors) <- model_names
names(model_confussions) <- model_names
model_errors
model_confussions
```
