---
title: "Trabajo 3: AJUSTE DE MODELOS LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
---

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")

```

#Clasificación.

## Comprensión del problema a resolver.

Para el problema de clasificación hemos elegido la base de datos de *South African Heart Disease*, que almacena una muestra recapituladora de hombres en alto riesgo de cardiopatía en la región de Western Cape, en Sudáfrica. Hay dos casos de CHD (0 o 1). Algunos de los hombres que tienen CHD positivo, se han sometido a un tratamiento de reducción de la presión sanguínea, siendo los datos que aparecen en la muestra posteriores a estos tratamientos. Estos datos datan de 1983.

Nuestra base de datos consta de 462 muestras donde cada una de ellas cuenta con 10 atributos, uno de ellos la variable de respuesta. Los diferentes atributos a tratar son:

- **sbp:** presión arterial sistólica. Toma valores entre 101 y 218, siendo la media 138.3. 
- **tobacco:** tabaco acumulativo (en kg). Toma como valor mínimo 0 y máximo 31.2. En este caso la media es de 3.6356.
- **ldl:** lipoproteína de baja densidad (colesterol). Toma valores entre 0.98 y 15.330, siendo la media 4.74.
- **adiposidad:** adiposi Tomando valores entre 6.74 y 42.49, siendo la media de los valores 25.41.
- **famhist:** historial familiar de cardiopatías. Toma como valores \{Present, Absent\}, habiendo del primer tipo 270 y del segundo 192.
- **typea:** Personalidad Tipo-A (mide el grado de estrés en el día a día). Toma valores entre 13 y 78, siendo la media 53.1.
- **obesity:** Obesidad. Toma valores entre 14.7 y 46.58, siendo la media 26.04.
- **alcohol:** Actual consumición de alcohol. Toma valores entre 0 y 147.19, encontrándose el valor medio en 17.04.
- **age:** Edad de los hombres al comienzo de las pruebas. Se encuentra entre 15 y 64 años, siendo la edad media 42.82.
- **chd:** Variable de respuesta, indica si se tiene o no alguna cardiopatía. Toma como valores \{0,1\}, siendo la media 0.3463.

```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #PREPARACIÓN DE LOS DATOS
    sahd <- read.table("./data/SAHD", sep=",",head=T, row.names = 1)
    summary(sahd)

```

## Preprocesado de datos.

Como podemos observar en el breve estudio de los diferentes atributos que vamos a trabajar, cada uno toma valores en una franja muy diferente, lo que hace primar unos atributos sobre otros en los métodos basados en distancias. Además, algunos atributos presentan una gran asimetría, lo cual también es conveniente evitar. Por estos motivos y otros más que estudiaremos a continuación, tenemos que preprocesar los datos.

### Modificación de los atributos cualitativos.
Tenemos que convertir los atributos cualitativos en atributos numéricos para que las funciones que usemos más adelante puedan trabajar con ellos. En nuestro problema concreto, solo contamos con un atributo cualitativo: *famhist*, que toma los valores \{Present, Absent\}. Convertimos este atributo en el atributo *present_famhist*, que tomará el valor 1, cuando *famhist* tomaba el valor Present y 0 en el otro caso. Así, el atributo *present_famhist* tomará valores en \{0,1\}.

```{r echo = FALSE, warning=FALSE, results = FALSE, message = FALSE}

    #PREPROCESAMIENTO DE LOS DATOS

    #Paso 1: Modificamos las variables cualitativas (el programa no sabe bien cómo tratarlas)
    #La única variable cualitativa es famhist = {present, absent}
    sahd[,5] <- ifelse(sahd[,5]=='Present',1,0)
    colnames(sahd) <- c( 'sbp', 'tobacco', 'ldl', 'adiposity', 'present_famhist', 'typea', 'obesity', 'alcohol', 'age','chd')
    
    #Para ir explicando una a una las transformaciones
    sahd_aux <- sahd
    
    attach(sahd_aux)
    #pairs(~ sbp + tobacco + ldl + adiposity + present_famhist +typea + obesity + alcohol + age, data= sahd, col = chd+3)
```

Para el resto de pasos, utilizaremos una función llamada *preProcess()*, la cual terminará con el preprocesado de los datos. Esta función realizará los siguientes cambios:

### Tratamiento de la asimetría con BoxCox.

Como ya hemos comentado anteriormente, hay varios atributos que presentan una alta asimetría, lo cual podría hacer que los métodos de predicción que apliquemos a continuación obtengan resultados peores. Para solucionar esto, utilizaremos un método llamado BoxCox. Este método se basa en la transformación potencial según un valor ($\lambda$) para aumentar la correlación entre las variables. Para elegir la mejor potencia (mejor $\lambda$), se busca entre los $\lambda$ que proporcionen un menor error residual. Aunque en la práctica, esto se realizará de manera automátcica con la función *preProcess()* vamos a ver cómo funciona en el caso de un atributo. 
Para que se vea mejor el funcionamiento, vamos a elegir el atributo que presente una mayor asimetría. Para ello, ordenamos los atributos en función de su asimetría:


```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    sahd_asymmetry <- apply(sahd_aux, 2, skewness)
    sahd_asymmetry <- sort(abs(sahd_asymmetry), decreasing = T)
    print(sahd_asymmetry)
    
```

Si dibujamos el histograma correspondiente al atributo con mayor asimetría, *alcohol* corroboramos que los datos se encuentran muy concentrados en los primeros valores que este atributo toma.

```{r, echo=F,warning=F}
    hist(alcohol)
```

Vamos a aplicar ahora el método *BoxCox* y veremos cómo mejora la simetría del atributo.

```{r, echo=F,warning=F}
    BoxCoxTrans(alcohol) #No se aplica transformación pues no se ha encontrado el parámetro
```

Como observamos, no se encuentra un $\lambda$ válido para realizar la transformación, y esto se debe a que entre los valores que toma el atributo se encuentra el 0, punto en el cual no está definida la función logaritmo. Para solucionar esto, realizamos una translación de los datos de la forma: datos = min(datos) + 1 + datos, para así conseguir que el mínimo de los datos se desplace a 1. Tras realizar esta transformación obtenemos:

```{r, echo=F,warning=F}
  correction <- 1
  c_alcohol <- alcohol + min(alcohol)+correction
  alcohol_trans <- BoxCoxTrans(c_alcohol)
  t_alcohol <- predict(alcohol_trans,c_alcohol)
  
  print("La asimetría del alcohol transformado es:")
  print(skewness(t_alcohol))
  
  par(mfrow = c(1,2))
  hist(alcohol)
  hist(t_alcohol)
  par(mfrow = c(1,1))
    
```

Para aplicar esta transformación a todos los atributos con el $\lambda$ correspondiente, pasaremos como parámetro al método $preProcess$ que realice el método $BoxCox$ y todo esto se hará de forma automática.

### Eliminación de atributos con PCA.

El algoritmo PCA (Principal Components Analysis) es un filtro no supervisado que es de gran utilidad cuando disponemos de una base de datos con un gran número de atributos, entre los que algunos pueden ser redundantes o irrelevantes. Como nuestra base de datos solo consta de 10 atributos, no es necesaria la aplicación de este método.

### Centrar y escalar.


<!--
No me queda muy claro como funciona pues en internet encuentro cosas raras <- JUANLU ME FIO MASD E TI.
-->

También es conveniente centrar y escalar las variables para que no prioricen unas sobre otras, dado que puede ser útil en algunos métodos que utilizaremos más adelante. Al escalar una variable lo que se está haciendo es dividir cada dato entre la desviación típica del conjunto de datos, transformándo así el conjunto de datos en un conjunto de varianza 1. En cuanto a centrar el conjunto de datos, lo que se hace es restar a cada dato la media del conjunto, transformándolo así en un conjunto de media 0. Al escalar y centrar a la vez, estamos aplicando la transformación $X \leftarrow (X-\mu)/\sigma$, normalizando así el conjunto a un conjunto de media 0 y varianza 1. Esto nos permite tener los atributos normalizados, y además manteniendo las mismas distribuciones entre los distintos atributos, como veremos más adelante en la regresión. El método *preProcess* se encargará de centrar las variables en 0 y de escalar las variables para tener varianza unitaria. Para ello, bastaría con pasar como argumento *scale* y *center* cuando llamemos al método *preProcess*.


### Llamada al método PreProcess.

Una vez entendidas las modificaciones que vamos a realizar a los datos, utilizaremos el método preProcess que se encarga de realizar todas estas modificaciones sobre el conjunto de datos pasado como argumento. Como ya hemos comentado, no vamos a aplicar el método *PCA*, luego la llamada quedaría de la siguiente forma:

```{r, echo=T, warning=F}

ObjetoTrans = preProcess(sahd[,names(sahd)!="chd"],method = c("BoxCox","center","scale"))
sahdTrans <- predict(ObjetoTrans,sahd)

```

## Los conjuntos de validación, training y test usados.

A continuación pasaremos a explorar los distintos modelos sobre los que resolver el problema de clasificación para nuestro conjunto de datos. El procedimiento de validación que usaremos consistirá en tomar múltiples veces distintos conjuntos de entrenamiento sobre nuestro conjunto de datos (una vez transformados), con los que aprenderemos el modelo. Usaremos el resto del conjunto como test para evaluar cómo de bien predice el modelo aprendido con nuevos datos. Las proporciones utilizadas serán del 70 % de los datos para train, y el 30 % para test.

```{r, echo=F, warning=F}
  #Paso 5: Preparación de conjunto de train/test, y de las etiquetas train/test
  sahd.data <- sahdTrans[,-ncol(sahdTrans)]
  sahd.label <- sahdTrans[,ncol(sahdTrans)]
  #sahd.label[sahd.label==0] <- -1 
  
  
  # ESTO NOS HACE FALTA YA ??? !!!
  train <- sample(nrow(sahd),0.7*nrow(sahd))
  sahd.train <- sahd.data[train,]
  sahd.test <- sahd.data[-train,]
  
  #Etiquetas
  lsahd.train <- sahd.label[train]
  lsahd.test <- sahd.label[-train]
  
```



```{r, echo=F,warning=F}
# Funciones para el cálculo del error
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

# Función para evaluar loserrores para un conjunto de modelos.
# Argumentos:
# models - lista de modelos
# test - conjunto de test
# ltrain - etiquetas de train
# ltest - etiquetas de test
# Devuelve: Matriz de num_modelos x 2. Las columnas representan (Ein, Etest)
testFamilies <- function(models, test, ltrain, ltest){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```


## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por las función `glm` (Generalized Linear Models) de `R`. Las familias que vamos a considerar para el ajuste son (!!!BUSCAR QUE SON !!!):

- **Binomial**, con link **logit**. Regresión logística (?)
- **Binomial**, con link **probit**.
- **Binomial**, con link **cauchit**.
- **Gaussiana**, con link **identity**. Regresión lineal (?)
- **Poisson**.
- **Quasi**.
- **Quasibinomial.**
- **Quasipoisson.**



```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

testFamilies <- function(models, test, ltrain, ltest){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```


```{r, echo=F,warning=F}

# Validación (una sola vez)
validateFamilies <- function(data,label){
  #Muestra
  train <- sample(nrow(data),0.7*nrow(data))
  
  #Datos
  data.train <- data[train,]
  data.test <-  data[-train,]
  
  #Etiquetas
  label.train <- label[train]
  label.test <- label[-train]
  
  #Modelos
  #binomial
  ml.binomial1 <- glm(chd ~ ., family = binomial(logit), data = data, subset=train)
  ml.binomial2 <- glm(chd ~ ., family = binomial(probit), data = data, subset=train)
  ml.binomial3 <- glm(chd ~ ., family = binomial(cauchit), data = data, subset=train)
  #gaussiano
  ml.gaussian1 <- glm(chd ~ ., family = gaussian(identity), data = data, subset=train)
  ml.gaussian2 <- glm(chd ~ ., family = gaussian(log), data = data, subset=train, start=rep(0, ncol(data)+1))
  #poisson
  ml.poisson1 <- glm(chd ~ ., family = poisson(log), data = data, subset=train)
  
  #quasi
  ml.quasi1 <- glm(chd ~ ., family = quasi(link = "identity", variance = "constant"), data = data, subset=train)

  #quasibinomial
  ml.quasibinomial1 <- glm(chd ~ ., family = quasibinomial(link = "logit"), data = data, subset=train)
  #quasipoisson
  ml.quasipoisson1 <- glm(chd ~ ., family = quasipoisson(link = "log"), data = data, subset=train)  

  models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
  
  testFamilies(models, data.test, label.train, label.test)
  
}

# Función para realizar multiples validaciones
repValidation <- function(rep,data,label){
  l <- replicate(n = rep, expr = validateFamilies(data,label))
  #l es una matriz 3D de rep x num_modelos x 2 (Ein,Eout)
  #l[,,i] -> experimento i-ésimo
  #[,i,] -> Ein / Eout de cada modelo en los distintos experimentos (i=1 Ein, i=2 Eout)
  #[i,,] -> Ein y Eout para el modelo i-ésimo en cada experimento
  apply(FUN = mean, X = l, MARGIN = c(1,2))
}

```

Una vez definidos los modelos y las funciones a usar, procedemos al ajuste de los distintos modelos y al análisis de sus errores:

```{r, echo=F, warning=F}
# Comprobamos qué método proporciona un menor error de validación con múltiples validaciones
repValidation(rep = 100, sahd.data, sahd.label)

# Confirmamos que el modelo que mejor generaliza es el la gaussiana2
ml.sahd = ml.gaussian2 <- glm(chd ~ ., family = poisson(log), data = sahd.data, subset=train, start=rep(0, ncol(sahd.data)+1))


```

Obtenemos que el modelo que mejores resultados proporciona es el de Poisson. Hay que destacar que los resultados de Poisson coinciden con los de quasipoisson, pero elegimos el de Poisson ser más conocido.

## Regularización

A continuación nos planteamos la necesidad de regularización, sobre el mejor modelo que hemos obtenido. Para ello utilizamos la regularización lasso. (explicar, lo dejo para luego !!!)

```{r, echo=F,warning=F}
  #library(glmnet)
  #El método para llamar a GLM con lasso (elasticnet regularization) es glmnet

  #En la documentación aqui parece que haya más families.

  #Mediante validación cruzada sacamos el mejor lambda

  testRegularization <- function(data, label, iter = 100){
      error1 <- 0
      error2 <- 0
      error3 <- 0
      
      for(i in 1:iter){
          train <- sample(nrow(data),0.7*nrow(data))
          data.train <- data[train,]
          data.test <- data[-train,]
          #Etiquetas
          ldata.train <- label[train]
          ldata.test <- label[-train]
          
          #Ponemos el link en las etiquetas pues a glmnet no se le puede pasar como argumento
          ml_lasso <- cv.glmnet(as.matrix(data.train), ldata.train, family="gaussian")
          ml = ml.gaussian2 <- glm(chd ~ ., family = gaussian(log), data = data, subset=train, start=rep(0, ncol(data)+1))
    
          error1 <- error1 + calculateEtest(ml_lasso,data.test,ldata.test,ml_lasso$lambda.min)[[1]]
          error2 <- error2 +calculateEtest(ml_lasso,data.test,ldata.test,ml_lasso$lambda.1se)[[1]]
          error3 <- error3 +calculateEtest(ml,data.test,ldata.test)[[1]]
      }
    
      print(error1/100)
      print(error2/100)
      print(error3/100)
  }

  testRegularization(sahd.data, sahd.label, 100)

  
  #ANTIGUO -> PERO PODRIAMOS HACER GRAFICAS Y ESO
  #Podemos ver los valores y varianzas
  #plot(ml_lasso)
  #plot(ml_lasso$glmnet.fit, xvar="lambda", label=TRUE)
  #Se puede usar lambda_min o lambda_1se, q es la más grande con varianza más pequeña
  #ml_lasso$lambda.min
  #ml_lasso$lambda.1se
  
  #Interpretación de los coeficientes -> log hazard ratios, ratio de riesgo.
  #Coeficiente positivo -> alto riesgo de suceso
  #Coeficiente negativo -> viceversa
  #Para q representen la "importancia" -> hay que escalarlos a varianza 1
  #Cuando tiene coeficiente 0 -> la ha quitado del problema (parece q a nosotros no nos quita nada)

  #Utilizamos lambda.1se pues nos quita más atributos.
  #coeffs.min <- coef(ml_lasso, s=ml_lasso$lambda.min)
  #coeffs.1se <- coef(ml_lasso, s = ml_lasso$lambda.1se)
  
  #Eliminamos los atributos del train  
  #coefficients.min <- coeffs.min[1:nrow(coeffs.min),]
  #coefficients.1se <- coeffs.1se[1:nrow(coeffs.1se),]
  #sahd.train_lasso = sahd.train[,abs(coefficients)>0]
  #Los eliminamos del test.
  #sahd.test_lasso = sahd.test[,abs(coefficients)>0]
  

```

## Optimización del número de atributos para el modelo seleccionado

Luego (!!!)

```{r, echo=F,warning=F}
  #nbest -> numero de subconjuntos de cada tamaño para almacenar (que nos devuelva solo el mejor de cada tamaño.)
  #library(leaps)

  #No le quito los atributos porque parece que va a confirmar lo que ya veníamos imaginando: PC2, PC6 y PC7 es caca

  #method = "exhaustive" para que no sea greedy.
  testing <- regsubsets(chd ~ ., data = sahd.data, nbest = 1, method = "exhaustive")

  #Obtenemos una tablita con los que es mejor coger.
  stesting <- summary(testing)
  stesting
  plot(testing)
  
  #Dibujar -> testing$ress
  plot(testing$ress)
  plot(stesting$cp)
  plot(stesting$bic)

  sahd.data <- sahd.data[,c(2,3,5,6,9)]
```

## Transformación de atributos

A continuación, nos planteamos la búsqueda de una transformación polinómica de los atributos para la cual nuestro modelo tenga una mayor capacidad de aprender y predecir nuestro conjunto de datos. Para ello utilizamos un algoritmo de búsqueda greedy. Para cada atributo, vamos eligiendo distintos exponentes y nos quedamos con el exponente que proporcione menor error de validación. Cuando llegamos al siguiente atributo, aplicamos el mismo procedimiento, manteniendo para los atributos anteriores el mejor exponente encontrado. Además, como cuando dos modelos proporcionan resultados similares siempre es mejor quedarse con el más simple, fijaremos una tolerancia para la cual, si no hay mejoras significativas en la nueva transformación, nos quedemos con la transformación mejor obtenida previamente, que tendrá un exponente menor y por tanto será más simple el ajuste.

```{r, echo=F,warning=F}
testPolyTransform <- function(data,label){
  #Muestra
  train <- sample(nrow(data),0.7*nrow(data))
  
  #Datos
  data.train <- data[train,]
  data.test <-  data[-train,]
  
  #Etiquetas
  label.train <- label[train]
  label.test <- label[-train]
  
  #Usamos el modelo ganador
  ml <- glm(chd ~., family = gaussian(log), data = data, subset =train,start=rep(0, ncol(data)+1))
  
  models <- list(ml)
  
  testFamilies(models, data.test, label.train, label.test)
  
}

testPolyTransformRep <- function(rep,data,label){
   l <- replicate(n = rep, expr = testPolyTransform(data,label))
  #l es una matriz 3D de rep x num_modelos x 2 (Ein,Eout)
  #l[,,i] -> experimento i-ésimo
  #[,i,] -> Ein / Eout de cada modelo en los distintos experimentos (i=1 Ein, i=2 Eout)
  #[i,,] -> Ein y Eout para el modelo i-ésimo en cada experimento
  apply(FUN = mean, X = l, MARGIN = c(1,2))
}

# Devuelve un vector de coeficientes para cada atributo
# tol: Nivel de tolerancia: si dos Eout se diferencian en menos de Eout, escogemos el coeficiente más simple
polynomialTransformGreedy <- function(data,label,max_coeff, tol = 0){
  transform <- data
  coeffs <- c()

  for(i in 1:ncol(transform)){
    bestEout <- 1
    bestInd <- 0
    for(j in 1:max_coeff){
      transform[,i] <- I(data[,i]^j)
      v <- testPolyTransformRep(100,transform,label)
      # Comparamos con Etest
      if(v[2] + tol < bestEout){
        bestEout <- v[2]
        bestInd <- j
      }
      cat("Attr = ",i,", Exp = ",j,", ","Ein = ", v[1], "Eout = ",v[2],"\n")
    }
    #Nos quedamos con el mejor coeficiente obtenido para el atributo
    transform[,i] <- I(data[,i]^bestInd)
    coeffs <- c(coeffs,bestInd)
  }
  # Por ser greedy con el mecanismo escogido el mejor Eout va a ser el de la última iteración
  list(coeffs,bestEout)
}
```

Aplicamos el algoritmo. Los resultados obtenidos son:

```{r, echo=F,warning=F}
l <- polynomialTransformGreedy(sahd.data,sahd.label,6, tol = 0.01)
cat("Vector de exponentes: ", l[[1]])
cat("Eout estimado: ",l[[2]])

```

Vemos que, para exponentes de hasta tamaño no se aprecian mejoras significativas con respecto a los coeficientes lineales iniciales.

# Regresión. 

Para el problema de regresión hemos elegido la base de datos de *Los Ángeles Ozone*, la cual se centra en medir el nivel de concentración de ozono en la atmósfera. Para ello, se realizaban 8 mediciones hechas diariamente en Los Ángeles durante el año 1976. Aunque la idea era obtener el nivel de ozono para todos los días del año, algunos datos se han perdido así que no contiene todos los días del año (en concreto contiene 330 días). La base de datos consta de 10 atributos que son los siguientes:

- **ozone**: Es la variable de respuesta, mide la elevación máxima del ozono. 
- **vh**: Vandenberg 500 mb Height
- **wind**: Velocidad del viento, medida en mph.
- **humidity:** Tanto por ciento de humedad.
- **temp:** Temperatura
- **ibh:** 
- **dpg:** Gradiente de presión de Daggot.
- **ibt:** 
- **vis:** La visibilidad medida en millas.
- **day:** Día del año en el que se realizó la medición.