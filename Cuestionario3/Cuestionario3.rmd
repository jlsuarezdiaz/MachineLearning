---
title: 'Aprendizaje Automático - Cuestionario 3'
author: "Juan Luis Su?rez D?az"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output:
  pdf_document:
    fig_caption: yes
    highlight: haddock
    toc: no
    toc_depth: 2
    includes:
      in_header: mystyles.sty
bibliography: references.bib
fontsize: 10pt
geometry: a4paper, top=2.5cm, bottom=2.5cm, left=3cm, right=3cm
lang: es-ES
linestretch: 1
csl: ieee.csl
---
<!--
Highlights: default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, null
-->
<!--
include-after: |2-
  * * *
  Esta obra se distribuye bajo una [Licencia Creative Commons Atribución-NoComercial-CompartirIgual 4.0 Internacional](http://creativecommons.org/licenses/by-nc-sa/4.0/).
  
mainfont: Arial
monofont: Source Code Pro

abstract: La navegación segura en Internet se extiende lentamente debido a numerosas
  dificultades en los procesos necesarios para su implementación. En este texto se
  explican y se analizan tres propuestas dirigidas a la difusión de las comunicaciones
  seguras y a la mejora de la certificación y la autenticación. Se observan las nuevas
  funcionalidades que traerá el próximo estándar HTTP/2 y se realiza un ejemplo de
  instalación en un servidor. De la misma forma, se presenta una autoridad de certificación
  automatizada, Let's Encrypt, y se demuestra su funcionamiento mediante las implementaciones
  de cliente y servidor del protocolo asociado ACME. Por último, se explica el mecanismo
  de verificación de identidad mediante certificados Convergence, frente a las autoridades
  de certificación, y se muestra un ejemplo de su uso.
  
(Cosas que puedo añadir a la cabecera)
-->

<!--

Añadir imagenes:

\begin{figure}[h]
\centering
\includegraphics[width=10 cm]{./images/1_1.png}
\caption{Instalación de phoronix suite.\label{fig:phinst}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=10 cm]{./images/1_2.png}
\caption{Lista de test disponibles.\label{fig:phtests}}
\end{figure}

-->

# Cuestión 1

*Considere un modelo de red neuronal con dos capas totalmente conectadas: $n_I$ unidades de entrada, $n_H$ unidades ocultas y $n_O$ unidades de salida. Considere la función de rror definida por $J(w) = \frac{1}{2}\sum_{k=1}^{n_O}(t_k-c_k)^2=\frac{1}{2}\|t-c\|^2$, donde el vector $t$ representa los valores de la etiqueta, $c$ los valores calculados por la red y $w$ los pesos de la red. Considere que las entradas a la segunda capa se calculan como $z_k = \sum_{j=0}^{N_H}{y_jw_{kj}}$, donde el vector $y$ representa la salida de la capa oculta.*

a) *Deducir con todo detalle la regla de adaptación de los pesos entre la capa oculta y la capa de salida.*

b) *Deducir con todo detalle la regla de adaptación de los pesos entre la capa de entrada y la capa oculta.*

*Usar $\theta$ para notar la función de activación.*

## Solución

La regla de adaptación de pesos para $w$, de acuerdo con el gradiente descendiente viene dada por $w \leftarrow w - \eta\nabla E_{in}(w)$, donde $\eta$ es una tasa de aprendizaje prefijada. Para calcular el gradiente debemos calcular por tanto las derivadas parciales respecto a cada elemento en el vector de pesos. Llamamos $x_i$ a los datos de entrenamiento y $t_i$ a las etiquetas. Llamamos $z_i$ a las entradas a la capa de salida y $c_i$ a las salidas de la capa de salida (y de la red neuronal). Para la capa oculta, llamamos $n_i$ a las entradas a la capa e $y_i$ a las salidas. Finalmente, llamamos $r_i$ a las salidas de la capa de entrada. Consideramos los pesos como $w_{ji}^{(k)}$, donde $j$ representa el índice de la conexión de salida, $i$ el índice de la conexión de entrada, y $k$ las capas entre las que se encuentra. Es decir, la distribución de la red es la que se muestra en la figura \ref{fig:red}. 

\begin{figure}[h]
\centering
\includegraphics[width=10 cm]{./images/redneuronal.jpg}
\caption{Distribución de la red neuronal.\label{fig:red}}
\end{figure}

Calculamos la regla de adaptación de pesos para los pesos entre la capa oculta y la de salida, $w_{ji}\equiv w_{ji}^{(2)}$. Es decir, buscamos el valor de $\frac{\partial J}{\partial w_{ji}}$. Aplicando la regla de la cadena, obtenemos:

\[\frac{\partial J}{\partial w_{ji}} = \frac{\partial J}{\partial c_j}\frac{\partial c_j}{\partial z_j}\frac{\partial z_j}{\partial w_{ji}}\]

Calculamos las derivadas parciales anteriores:

\[\frac{\partial J}{\partial c_j} = \frac{\partial }{\partial c_j}\frac{1}{2}\sum_{k=1}^{n_O}(t_k-c_k)^2 = \frac{\partial }{\partial c_j}\frac{1}{2}(t_j-c_j)^2=-(t_j-c_j)\]

Para derivar $c_j$ respecto de $z_j$, utilizamos que $c_j = \theta(z_j)$:

\[\frac{\partial c_j}{\partial z_j} = \frac{\partial }{\partial z_j}\theta(z_j) = \theta'(z_j)\]

Finalmente, para derivar $z_j$ respecto de $w_{ji}$ usamos que $z_j = \sum_{k=0}^{N_H}{y_kw_{jk}}$:

\[\frac{\partial z_j}{\partial w_{ji}} = \frac{\partial }{\partial w_{ji}}\sum_{k=0}^{N_H}{y_kw_{jk}}=\frac{\partial }{\partial w_{ji}}y_iw_{ji}=y_i\]

Por tanto, la regla de adaptación de pesos para cada peso entre la capa oculta y la de salida es:

\[w_{ji} \leftarrow w_{ji} +\eta(t_j-c_j)\theta'(z_j)y_i\]

Ahora calculamos la regla de adaptación de pesos para cada pesos entre la capa de entrada y la oculta, $w_{ji}\equiv w_{ji}^{(1)}$. Buscamos de nuevo el valor de $\frac{\partial J}{\partial w_{ji}}$. Aplicando la regla de la cadena, obtenemos:

\[\frac{\partial J}{\partial w_{ji}} = \frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial n_j}\frac{\partial n_j}{\partial w_{ji}}\]

Derivamos primero $J$ respecto de $y_j$:

\[\frac{\partial J}{\partial y_j} = \frac{1}{2}\frac{\partial}{\partial y_j}\sum_{k=1}^{n_O}(t_k-c_k)^2 = \frac{1}{2}\sum_{k=1}^{n_O}\frac{\partial}{\partial y_j}(t_k-c_k)^2\]

Por la regla de la cadena, $\frac{\partial}{\partial y_j} = \frac{\partial}{z_k}\frac{\partial z_k}{y_j}$, para cada $k = 1,\dots,n_O$. Aplicando esto a la expresión anterior, obtenemos:

\[\frac{\partial J}{\partial y_j} = \frac{1}{2}\sum_{k=1}^{n_O}\frac{\partial}{\partial z_k}(t_k-c_k)^2\frac{\partial z_k}{\partial y_j}\]

La expresión $\frac{\partial}{\partial z_k}(t_k-c_k)^2$ está calculada en el apartado anterior y vale $-(t_k-c_k)\theta'(z_k)$, mientras que $\frac{\partial z_k}{\partial y_j} = \frac{\partial}{\partial y_j}\sum_{i=0}^{N_O}y_iw_{ki} = \frac{\partial}{\partial y_j}y_jw_{kj} = w_{kj}^{(2)}$, es decir, el peso $w_{kj}$ entre las capas oculta y de salida, que ya hemos calculado en el apartado anterior. Por tanto:

\[\frac{\partial J}{\partial y_j} = -\sum_{k=1}^{n_O}(t_k-c_k)\theta'(z_k)w_{kj}^{(2)}\]

Finalmente, calculamos el resto de derivadas parciales de forma análoga al apartado anterior:

\[\frac{\partial y_j}{\partial n_j}=\frac{\partial}{\partial n_j}\theta(n_j) = \theta'(n_j)\]

\[\frac{\partial n_j}{\partial w_{ji}} = \frac{\partial}{\partial w_{ji}}\sum_{k=0}^{N_I}r_kw_{jk} = \frac{\partial}{\partial w_{ji}}r_iw_{ji} = r_i\]

Por tanto, hemos obtenido que:

\[\frac{\partial J}{\partial w_{ji}} = -\left[\sum_{k=1}^{n_O}(t_k-c_k)\theta'(z_k)w_{kj}^{(2)}\right]\theta'(n_j)r_i\]

Luego la regla de adaptación de pesos entre la capa de entrada y la oculta es:


\[ w_{ji} \leftarrow w_{ji} + \eta\left[\sum_{k=1}^{n_O}(t_k-c_k)\theta'(z_k)w_{kj}^{(2)}\right]\theta'(n_j)r_i\]


# Cuestión 2

*Tanto "bagging" como validación cruzada cuando se aplican sobre una muestra de datos nos permiten dar una estimación del error de un modelo ajustado a partir de dicha muestra de datos. Discuta cuál de los dos métodos considera que obtendrá una mejor estimación del error. Especificar con precisión las razones.*

## Solución


# Cuestión 3

*Considere que dispone de un conjunto de datos linealmente separable. Recuerde que una vez establecido un orden sobre los datos, el algoritmo perceptron encuentra un hiperplano separador iterando sobre los datos y adaptando los pesos de acuerdo al algoritmo*

\begin{figure}[h]
\centering
\includegraphics[width=10 cm]{./images/pla.png}
\end{figure}

*Modificar este pseudo-código para adaptarlo a un algoritmo simple de SVM, considerando que en cada iteración adaptamos los pesos de acuerdo al caso peor clasificado de toda la muestra. Justificar adecuadamente el resultado, mostrando que al final del entrenamiento solo estaremos adaptando los vectores soporte.*


## Solución


# Cuestión 4

*Considerar un modelo SVM y los siguientes datos de entrenamiento: Clase-1: $\{(1,1),(2,2),(2,0)\}$, Clase-2: $\{(0,0),(1,0),(0,1)\}$*

a) *Dibujar los puntos y construir por inspección el vector de pesos para el hiperplano óptimo y el margen óptimo.*

b) *¿Cuáles son los vectores soporte?*

c) *Construir la solución en el espacio dual. Comparar la solución con la del apartado (a)*

## Solución

Asignamos la etiqueta 1 a los datos de la clase 1 y la etiqueta -1 a los datos de la clase 2. Los puntos se distribuyen de la siguiente forma:

```{r, echo=F,warning=F}
data <- matrix(nrow = 6, ncol = 2)
data[1,] <- c(1,1)
data[2,] <- c(2,2)
data[3,] <- c(2,0)
data[4,] <- c(0,0)
data[5,] <- c(1,0)
data[6,] <- c(0,1)

label <- c(1,1,1,-1,-1,-1)

plot(data,col = label + 3, pch = label + 17)

```

Buscamos el hiperplano óptimo que separa los datos, y para ello buscamos un vector de pesos $(b w_1 w_2)$, con $w = (w_1 w_2)$ que minimice la función $\frac{1}{2}w^Tw = \frac{1}{2}(w_1^2+w_2^2)$ sujeta a las restricciones $y_n(w^Tx_n + b) \ge 1$, donde cada $x_n$ representa a un dato y cada $y_n$ su etiqueta asociada.

Las restricciones que obtenemos para los puntos dados son las siguientes:

\begin{eqnarray}
  (1,1) & \to & w_1+w_2+b \ge 1 \\
  (2,2) & \to & 2w_1 + 2w_2 + b \ge 1 \\
  (2,0) & \to & 2w_1 + b \ge 1 \\
  (0,0) & \to & -b \ge 1 \\
  (1,0) & \to & -w_1 - b \ge 1 \\
  (0,1) & \to & -w_2 - b \ge 1 
\end{eqnarray}

Sumando las inecuaciones (1) y (5), obtenemos que $w_2 \ge 2$, y sumando las inecuaciones (1) y (6) $w_1 \ge 2$. Además se verifica que $\frac{1}{2}w^Tw$ sujeta a que $w_1,w_2 \ge 2$ alcanza su mínimo cuando $w_1=w_2=2$. Tomando estos valores para $w$ las desigualdades anteriores quedan:

\begin{eqnarray}
  (1,1) & \to & 4+b \ge 1 \\
  (2,2) & \to & 8 + b \ge 1 \\
  (2,0) & \to & 4 + b \ge 1 \\
  (0,0) & \to & b \le -1 \\
  (1,0) & \to & b \le -3 \\
  (0,1) & \to & b \le -3 
\end{eqnarray}

Por tanto, observamos que tomando $b = -3$ se satisfacen todas las restricciones, y además se minimiza $\frac{1}{2}w^Tw$ sobre estas restricciones. Por tanto, el vector de pesos asociados al hiperplano es $(b w_1 w_2)=(-3 2 2)$ y por tanto el hiperplano óptimo que separa los datos viene dado por la ecuación $2u + 2v = 3$. En la siguiente gráfica se muestra la recta obtenida:

```{r, echo=F,warning=F}
coefs_recta_explicita <- function(coefs_recta_impl){
  return(c(-coefs_recta_impl[1]/coefs_recta_impl[2],-coefs_recta_impl[3]/coefs_recta_impl[2]))
}

plot(data,col = label + 3, pch = label + 17)

r <- coefs_recta_explicita(c(2,2,-3))
abline(r[2],r[1],col = "green")

```

Finalmente, el margen óptimo viene dado por la fórmula $M = \frac{1}{\|w\|} = \frac{1}{\sqrt{w_1^2+w_2^2}}=\frac{1}{\sqrt{8}}$

b)

Los vectores soporte son aquellos para los que la distancia al hiperplano coincide con el margen, o equivalentemente, aquellos para los que se da la igualdad en la restricción asociada. Para ello,  para cada dato y con el vector de pesos obtenido $(b w_1 w_2)=(-3 2 2)$, calculamos $y_n(w^Tx_n + b)$ y vemos si es igual a 1.
 

\begin{eqnarray}
  (1,1) & \to & w_1+w_2+b = 1 \\
  (2,2) & \to & 2w_1 + 2w_2 + b = 5 \\
  (2,0) & \to & 2w_1 + b = 1 \\
  (0,0) & \to & -b = 3 \\
  (1,0) & \to & -w_1 - b = 1 \\
  (0,1) & \to & -w_2 - b = 1 
\end{eqnarray}

Por tanto, obtenemos que los vectores soporte son $(1,1),(2,0),(1,0)$ y $(0,1)$. En la gráfica anterior se comprueba que son los más cercanos al hiperplano.



# Cuestión 5

*Una empresa está valorando cambiar su sistema de proceso de datos, para ello dispone de dos opciones, la primera es adquirir dos nuevos sistemas idénticos al actual a 200.000 euros cada uno, y la segunda consiste en adquirir un sistema integrado por 800.000 euros. Las ventas que la empresa estima que tendrá a lo largo de la vida útil de cualquiera de sus equipos son de 5.000.000 de euros en el caso de positivo, a lo que la empresa le asigna una probabilidad de que suceda del 30 %, en caso contrario, las ventas esperadas son de 3.500.000 euros. ¿Qué opción debería de tomar la empresa?*

## Solución





# Cuestión 6

*El método de Boosting representa una forma alternativa en la búsqueda del mejor clasificador respecto del enfoque tradicional implementado por los algoritmos PLA, SVM, NN, etc.*

a) *Identifique de forma clara y concisa las novedades del enfoque.*
b) *Diga las razones profundas por las que la técnica funciona produciendo buenos ajustes (no ponga el algoritmo).*
c) *¿Cuál es su capacidad de generalización comparado con SVM?*

## Solución


# Cuestión 7

*¿Cuál es a su criterio lo que permite a clasificadores como Random Forest basados en un conjunto de clasificadores simples aprender de forma más eficiente? ¿Cuáles son las mejoras que introduce frente a los clasificadores simples? ¿Es Random Forest óptimo en algún sentido? Justifique con precisión las contestaciones.*

## Solución



