---
title: "Trabajo 3"
author: "Nuria Rodríguez Barroso"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
---

```{r echo = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)
```

#Clasificación.

## Comprensión del problema a resolver.

```{r echo = FALSE, warning=FALSE}
  
    #PREPARACIÓN DE LOS DATOS
    sahd <- read.table("./data/SAHD", sep=",",head=T, row.names = 1)
    summary(sahd)
    

    #PREPROCESAMIENTO DE LOS DATOS

    #Paso 1: Modificamos las variables cualitativas (el programa no sabe bien cómo tratarlas)
    #La única variable cualitativa es famhist = {present, absent}
    sahd[,5] <- ifelse(sahd[,5]=='Present',1,0)
    colnames(sahd) <- c('sbp', 'tobacco', 'ldl', 'adiposity', 'present_famhist', 'typea', 'obesity', 'alcohol', 'age', 'chd')
    
    attach(sahd)
    pairs(~ sbp + tobacco + ldl + adiposity + present_famhist +typea + obesity + alcohol + age, data= sahd, col = chd+3)
```

## Preprocesado de datos.

```{r echo = FALSE, warning=FALSE}
    
    #Paso 2: No hay columnas que contengan status ni redundantes

    #Paso 3: Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
    library("e1071")
    
    #skewness -> medida de la asimetría de los datos
    skewness(sbp)
    skewness(tobacco)
    skewness(ldl)
    skewness(adiposity)
    skewness(typea)
    skewness(obesity)
    skewness(alcohol)
    skewness(age)
    skewness(present_famhist)

    #Los datos más asimétricos son -> tobacco y alcohol
    hist(tobacco)
    hist(alcohol)
    
```


```{r, echo=F,warning=F}
    
    #Ordenamos las columnas por asimetria
    sahd_asymmetry <- apply(sahd, 2, skewness)
    sort(abs(sahd_asymmetry), decreasing = T)
    
    #Una vez hemos hallado los datos con asimetría alta, aplicamos la transformación, para ello
    library("caret")
    
    #Lo aplico a los que tienen skewness > 1 
    #BoxCoxTrans no hace la transformación, devuelve el parámetro
    BoxCoxTrans(alcohol) #No se aplica transformación pues no se ha encontrado el parámetro
    BoxCoxTrans(tobacco) #No se aplica transformación pues no se ha encontrado el parámetro
    ldl_trans <- BoxCoxTrans(ldl) #Se aplica transformación con lambda = 0
    sbp_trans <- BoxCoxTrans(sbp) #Lambda estimado -1.8
    
    #Transformamos los datos para los que no se ha encontrado lambda añadiendo una constante
    correction <- 1
    c_alcohol <- alcohol + min(alcohol)+correction
    c_tobacco <- tobacco + min(tobacco)+correction
    alcohol_trans <- BoxCoxTrans(c_alcohol)
    tobacco_trans <- BoxCoxTrans(c_tobacco)
    
    #Aplicamos la transformación a los que han devuelto un lambda
    #predict(ldl_trans, ldl) #Aplicamos sobre solo los 10 primeros? no entiendop
    #predict(sbp_trans, sbp)
    
    #Dibujamos el histograma de los datos transformados 
    #hist(predict(ldl_trans, ldl))
    #hist(predict(sbp_trans, head(sbp))) 
    #hist(predict(alcohol_trans,alcohol))
    
    # Veamos cómo han cambiado los histogramas de los datos más asimétricos
    t_alcohol <- predict(alcohol_trans,c_alcohol)
    t_tobacco <- predict(tobacco_trans,c_tobacco)
    t_ldl <- predict(ldl_trans, ldl)
    t_sbp <- predict(sbp_trans, sbp)
    
    skewness(t_alcohol)
    skewness(t_tobacco)
    par(mfrow = c(1,2))
    hist(alcohol)
    hist(t_alcohol)
    hist(tobacco)
    hist(t_tobacco)
    hist(ldl)
    hist(t_ldl)
    hist(sbp)
    hist(t_sbp)
    par(mfrow = c(1,1))
```

```{r, echo=F, warning=F}
    
    #Sustituímos los datos que tenían gran asimetría por los transformados
    ldl <- t_ldl
    sbp <- t_sbp
    alcohol <- t_alcohol
    tobacco <- t_tobacco
    
```

```{r, echo=F, warning=F}
    #Paso 4: Eliminación de atributos 
    #center=TRUE -> indicamos que queremos que las variables sean desplazadas de forma que estén centradas en 0.
    #scale=TRUE -> escalar las variables para que tengan varianza 1 antes del análisis.
    pcaObject <- prcomp(sahd,center=TRUE,scale=TRUE)

    # Centros utilizados (?) no lo entiendo  muy bien
    #head(pcaObject$center)
    #Si pones el head te saca solo los 10 primeros
    pcaObject$center

    #Peso en porcentajes de la varianza de cada atributo
    porcentVariance = pcaObject$sd^2/sum(pcaObject$sd^2)*100
    porcentVariance
    sum(porcentVariance)
    
    # Datos tras rotar y escalar 
    # Cada PCi no sé qué representa
    pcaObject$x
    
    #Atributos junto a su varianza
    plot(pcaObject,type="l")
    
    pcaObject$rotation
    

```

```{r, echo=F, warning=F}
# Realizamos todo el preprocesamiento directamente con preProcess (ejecutando solo el chunk de lectura de datos para hacer esto)
#library(caret)

ObjetoTrans = preProcess(sahd[,names(sahd)!="chd"],method = c("BoxCox","center","scale","pca"),thres = 0.9)

# El parámetro thres indica cuántas componentes hacen falta para explicar la fracción thres de los datos
# 1 -> 2 atributos ??????
# 0.95 -> 8 atributos
# 0.9 -> 7 atributos
# 0.8 -> 6 atributos

#THRES ->  cota del porcentaje de varianza acumulativa retenido por PCA.
ObjetoTrans

sahdTrans <- predict(ObjetoTrans,sahd)
dim(sahdTrans)

```

## Preparación de conjutos de training, validación y test.

```{r, echo=F, warning=F}
  #Paso 5: Preparación de conjunto de train/test, y de las etiquetas train/test
  sahd.data <- sahdTrans[,-1]
  sahd.label <- sahdTrans[,1]
  #sahd.label[sahd.label==0] <- -1 
  
  train <- sample(nrow(sahd),0.7*nrow(sahd))
  sahd.train <- sahd.data[train,]
  sahd.test <- sahd.data[-train,]
  
  #Etiquetas
  lsahd.train <- sahd.label[train]
  lsahd.test <- sahd.label[-train]
  
```

## Selección de clases de funciones a usar.

```{r, echo=F,warning=F}
# REGRESIÓN LOGÍSTICA
calculateEtest <- function(ml, test, ltest){
  #Cálculo de probabilidades
  ml.prob_test = predict(ml, test, type="response")

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  
  cat(length(ml.pred_test))
  cat(length(ltest))
  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  print(ml)
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.pred_train))
}

#binomial
ml.binomial1 <- glm(chd ~ ., family = binomial(logit), data = sahd.data, subset=train)
ml.binomial2 <- glm(chd ~ ., family = binomial(probit), data = sahd.data, subset=train)
ml.binomial3 <- glm(chd ~ ., family = binomial(cauchit), data = sahd.data, subset=train)

#gaussiana
ml.gaussian1 <- glm(chd ~ ., family = gaussian(identity), data = sahd.data, subset=train)
ml.gaussian2 <- glm(chd ~ ., family = gaussian(log), data = sahd.data, subset=train, start=rep(0, ncol(sahd.data)+1))
#ml.gaussian3 <- glm(chd ~ ., family = gaussian(inverse), data = sahd.data, subset=train,start=rep(, ncol(sahd.data)+1)) <- no converge
  
#poisson
ml.poisson1 <- glm(chd ~ ., family = poisson(log), data = sahd.data, subset=train)
#ml.poisson2 <- glm(chd ~ ., family = poisson(identity), data = sahd.data, subset=train, start=rep(0,ncol(sahd.data)+1)) 
#ml.poisson1 <- glm(chd ~ ., family = poisson(sqrt), data = sahd.data, subset=train, start=rep(1, ncol(sahd.data)+1))

#quasi
ml.quasi1 <- glm(chd ~ ., family = quasi(link = "identity", variance = "constant"), data = sahd.data, subset=train)
#ml.quasi2 <- glm(chd ~ ., family = quasi(link = "sqrt"), data = sahd.data, subset=train, start=rep(1, ncol(sahd.data)+1)) <- hay mas, todas con el mismo error
  
#quasibinomial
ml.quasibinomial1 <- glm(chd ~ ., family = quasibinomial(link = "logit"), data = sahd.data, subset=train)

#quasipoisson
ml.quasipoisson1 <- glm(chd ~ ., family = quasipoisson(link = "log"), data = sahd.data, subset=train)  

models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)

testFamilies <- function(models, test, ltrain, ltest){
  #print(models)
  results <- matrix(nrow=length(models), ncol = 2)

  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
  
testFamilies(models, sahd.test, lsahd.train, lsahd.test)

ml.sahd = ml.poisson1;
#cat("Ein con el modelo LR: "); 
#print(ml1$call)
#print(ml1.Ein)
```

Por tanto, la familia de funciones elegidas es -> la que genera menor Etest -> ml.poisson1 = poisson(log)

## Regularización.

Hay que mirar si hace falta regularización -> ¿Se realiza mucho aprendizaje?

Lasso -> Trata de evitar el sobreaprendizaje penalizando los coeficientes grandes -> Ventaja : además nos simplifica el modelo.

Si suponemos que sí -> R proporciona una función llamada regresión lasso, el funcionamiento consiste en aplicar regresión y cuando los valores están muy cercanos a 0, los trunca directamente a 0.

```{r, echo=F,warning=F}
  #library(glmnet)
  #El método para llamar a GLM con lasso (elasticnet regularization) es glmnet

  #En la documentación aqui parece que haya más families.

  #Mediante validación cruzada sacamos el mejor lambda
  ml_lasso <- cv.glmnet(as.matrix(sahd.train), lsahd.train, family="gaussian")

  #Podemos ver los valores y varianzas
  plot(ml_lasso)
  plot(ml_lasso$glmnet.fit, xvar="lambda", label=TRUE)
  #Se puede usar lambda_min o lambda_1se, q es la más grande con varianza más pequeña
  ml_lasso$lambda.min
  ml_lasso$lambda.1se
  
  #Interpretación de los coeficientes -> log hazard ratios, ratio de riesgo.
  #Coeficiente positivo -> alto riesgo de suceso
  #Coeficiente negativo -> viceversa
  #Para q representen la "importancia" -> hay que escalarlos a varianza 1
  #Cuando tiene coeficiente 0 -> la ha quitado del problema (parece q a nosotros no nos quita nada)

  #Utilizamos lambda.1se pues nos quita más atributos.
  coeffs <- coef(ml_lasso, s=ml_lasso$lambda.1se)
  
  #Eliminamos los atributos del train  
  coefficients <- coeffs[2:nrow(coeffs),]
  sahd.train = sahd.train[,abs(coefficients)>0]
  #Los eliminamos del test.
  sahd.test = sahd.test[,abs(coefficients)>0]
```

Parece ser que para un umbral medio pequeño nos quedaríamos con todas, luego no hay que aplicar regularización.

Usando lambda1se si-> Nos elimina 3 características.

## Definición del modelo.

Usar regsubsets

```{r, echo=F,warning=F}
  #nbest -> numero de subconjuntos de cada tamaño para almacenar (que nos devuelva solo el mejor de cada tamaño.)
  #library(leaps)

  #No le quito los atributos porque parece que va a confirmar lo que ya veníamos imaginando: PC2, PC6 y PC7 es caca
  testing <- regsubsets(chd ~ ., data = sahd.data, nbest = 1)

  #Lo hacemos teniendo en cuenta los otros
  sahd.data = sahd.data[, abs(coefficients) > 0]
  testing2 <- regsubsets(chd ~., data = sahd.data, nbest=1)

  #Obtenemos una tablita con los que es mejor coger.
  summary(testing)
  summary(testing2)
  
  #Más negrito cuanto más aparezca?
  plot(testing2)
  
  #Vamos a comprobar que es el que menor error produce (esto es solo probando)
  ml.1PC <-  glm(chd ~ present_famhist, family = poisson(log), data = sahd.data, subset=train)
  calculateEtest(ml.1PC, sahd.test, lsahd.test)
  calculateEin(ml.1PC, lsahd.train)
  
  length(train)
  
  
  rows(sahd.test)
```

## Selección y ajuste del modelo final.


## Estimación del $E_out$.

## Estudio de la calidad del modelo.

#Regresión

```{r echo = FALSE}
    #PREPARACIÓN DE LOS DATOS
    ozone.df <- read.table("./data/ozone", sep=",",head=T)
    summary(ozone.df)

    #PREPROCESAMIENTO DE LOS DATOS <- Esto hay que hacerlo también en la regresión? No lo tengo claro
    dim(ozone.df)
    

    #Paso 1: No hay variables cualitativas -> no problemo
    
    #Paso 2: No hay columnas que tengan "Status" ni redundantes (?)
    
    #Paso 3: Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
    
    #Paso 4: Eliminación de atributos (en nuestro modelo no hay demasiados (creo))
    

``` 

```{r, echo=F,warning=F}
    #Paso 5: Preparación de conjunto de train/test
    train <- sample(nrow(ozone.df), 0.7*nrow(ozone.df))
    ozone.data <- ozone.df[,-1]
    ozone.label <- ozone.df[,1]
    
    ozone.train <- ozone.data[train,]
    ozone.test <- ozone.data[-train,]
    
    lozone.train <- ozone.label[train]
    lozone.test <- ozone.label[-train]

    #Ozone es lo que vamos a estimar
    attach(ozone.df)
    pairs(~ozone + vh + wind + humidity + temp + ibh + dpg + ibt + vis + doy, data= ozone.df)
    mr1 = lm(ozone ~ vh + wind + humidity + temp + ibh + dpg + ibt + vis + doy, data=ozone.df, subset=train)
    
    ObjetoTrans.ozone = preProcess(ozone.df[,names(ozone.df)!="ozone"],method = c("BoxCox","center","scale"),thres = 0.95)
    ozoneTrans = predict(ObjetoTrans.ozone, ozone.df)
    pairs(ozoneTrans)

    
    #Cálulo del Ein
    mr1.Ein <- mean(mr1$residuals^2)
    
    mr1.pred_train = predict(mr1,type = "response")
    mr1.Ein2 <- mean((mr1.pred_train-lozone.train)^2)
    # Salen iguales Ein y Ein2, asi que perfe
    
    #Cálculo del Etest
    mr1.pred_test = predict(mr1, ozone.test, type="response")
    mr1.Etest <- mean((mr1.pred_test-lozone.test)^2)
    
    #plot(doy,ozone, main = "doy vs mpg")
    #abline(m1$coefficients[c(1,10)])
    

```


```{r, echo=F,warning=F}
# Pruebo con una transformación

# Correlaciones aparentes (por el dibujo):
# vh - Potencial e incluso exponencial
# wind - Poca correlación, quizás cuadrática
# humidity - Poca correlación, quizás cuadrática o lineal
# temp - Mucha correlación, lineal o quizás cuadrática
# ibh - Poca correlación
# dpg - Cuadrática
# ibt - Potencial e incluso exponencial
# vis - Poca correlación
# doy - Cuadrática

mr2 = lm(ozone ~ I(vh^2) + temp + I(dpg^2) + I(ibt^2) + I(doy^2), data=ozone.df, subset=train)

#Cálulo del Ein
    mr2.Ein <- mean(mr2$residuals^2)
    
    #Cálculo del Etest
    mr2.pred_test = predict(mr2, ozone.test, type="response")
    mr2.Etest <- mean((mr2.pred_test-lozone.test)^2)
    

```


```{r, echo=F,warning=F}
# Idea para estimar el comportamiento del ozono respecto a cada dato

# - Elaboramos las tablas (ozono,dato) respecto a cada dato ordenadas por ozono
# - Calculamos el vector de cocientes (ozono[i]-ozono[i-1])/(dato[i]-dato[i-1])
# - Estudiamos el comportamiento del vector cociente

dd <- matrix(ncol = ncol(ozone.df)-1,nrow = nrow(ozone.df)-1)

for(j in 2:ncol(ozone.df)){
  ddm <- matrix(c(ozone,ozone.df[,j]),ncol = 2)
  for(i in 2:nrow(ddm)){
    dd[i-1,j-1] <- (ddm[i,2]-ddm[i-1,2])/(ddm[i,1]-ddm[i-1,1])
  }
}


```

