---
title: "PROYECTO FINAL: AJUSTE DE MODELOS NO-LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
toc: yes
---

<!--
  6- Por qué SVM tarda mucho -> Porque los datos están muy juntos -- lo de espradao y etar andando
  7- boosting -> cuidad con el ruido -> regularizar
  8- En neuronas: Tasa de aprendizaje, mini/max, funcion de error (gradiente estocastic o no)
-->
\clearpage

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")
    library("DMwR")
    library("neuralnet")
    library("nnet")
    library("randomForest")
    library("gbm")
    library("adabag")
    library("maboost")
```


## Comprensión del problema a resolver: Human Activity Recognition Using Smartphones Data Set.

La base de datos elegida para el problema recoge la información de un grupo 30 voluntarios con edades comprendidas entre 18-48 años. Cada uno de estos individuos tenía que realizar una serie de actividades con el teléfono móvil enganchado en su cintura. Así, utilizando el dispositivo se podían registrar la aceleración lineal y angular en 3-ejes. Los experimentos se grabana en vídeo para poder etiquetar los datos de forma manual. De entre los datos recogidos, se seleccionaron de forma aleatoria el 70\% de esto para hacer de conjunto de training mientras que el otro 30\% se encargaba de recoger los datos para el test. 
Así, la base de datos se compone de 7352 muestras en el conjunto de train y 2947 en el conjunto de test, formando un total de 10299 muestras recogidas de estos 30 voluntarios y se consideran un total de 561 atributos.

Observamos el resumen de los diez primeros atributos:

```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #LECTURA DE LOS DATOS
    har.train <- read.table("./UCIHARDataset/train/X_train.txt", sep="", head = F)
    lhar.train <- read.table("./UCIHARDataset/train/y_train.txt", sep="", head = F)
    har.data.train <- cbind(lhar.train, har.train)
    
    har.test <- read.table("./UCIHARDataset/test/X_test.txt", sep="", head = F)
    lhar.test <- read.table("./UCIHARDataset/test/y_test.txt", sep="", head = F)
    har.data.test <- cbind(lhar.test, har.test)


    print(summary(har.train[,1:10]))
```

En cuanto a las etiquetas, nos encontramos ante un problema de clasificación multiclase, dado que las etiquetas toman valores enteros en el intervalo [1,6]. Cada etiqueta clasifica el movimiento registrado por el dispositivo móvil y lo clasifica en seis tipos:
  1. WALKING
  2. WALKING_UPSTAIRS
  3. WALKING_DOWNSTAIRS
  4. SITTING
  5. STANDING
  6. LAYING
  
Podemos observar fijándonos en lo que representa cada etiqueta que los tres primeros valores de las etiquetas hacen referencia a actividades muy similares, relacionadas con andar, mientras que las otras tres tienen la misma propiedad, representando estados de no movimiento. Podemos intuir por tanto, en vista a lo que representan las distintas clases, que los datos de las tres primeras clases serán fácilmente separables de los datos de las tres restantes, si las medidas que se han tomado son representativas del movimiento, mientras que a priori será más complicado distinguir dentro de cada grupo de clases, cuál será la asignación correcta.

Al tratarse de un problema de clasificación multiclase, para la aplicación de aquellos métodos que precisen de clasificación binaria para un buen funcionamiento utilizaremos la técnica de *One vs One*, la cual consistirá en dividir nuestro problema de clasificación multiclase en $\binom{6}{2} = 15$ problemas de clasificación binaria de la forma clase_i vs clase_j con $i,j \in [1,6]$,

Así, el cálculo del error para cada modelo propuesto consistirá en el número de errores cometidos. Esto es, número de muestras con etiqueta asignada de forma errónea. Para aquellos modelos que precisen de una adaptación a subproblemas de clasificación binaria, obtendremos quince vectores con la probabilidad de asignación de cada una de las seis etiquetas disponibles. Por tanto, le asignaremos a cada elemento la etiqueta que obtenga una mayor probabilidad, calculando el error en este caso de la misma forma que para el resto de modelos.

## Preprocesado de datos.

  En primer lugar, observamos que nuestra base de datos no contiene valores perdidos (NaNs), por lo cual no será necesario el trato especial de estos valores. Lo mismo ocurre con las variables cualitativas.
  
  En cuanto al desbalanceo, observemos el porcentaje de elementos de cada clase en el train y después en el test:

```{r, echo=F,warning=F}
  #Estudiamos si hay desbalanceo
  print("El porcentaje de elementos con etiqueta 1 en el train es: ")
  print(100*sum(lhar.train == 1)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 2 en el train es: ")
  print(100*sum(lhar.train == 2)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 3 en el train es: ")
  print(100*sum(lhar.train == 3)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 4 en el train es: ")
  print(100*sum(lhar.train == 4)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 5 en el train es: ")
  print(100*sum(lhar.train == 5)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 6 en el train es: ")
  print(100*sum(lhar.train == 6)/nrow(lhar.train))
```


```{r, echo=F,warning=F}

  print("El porcentaje de elementos con etiqueta 1 en el test es: ")
  print(100*sum(lhar.test == 1)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 2 en el test es: ")
  print(100*sum(lhar.test == 2)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 3 en el test es: ")
  print(100*sum(lhar.test == 3)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 4 en el test es: ")
  print(100*sum(lhar.test == 4)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 5 en el test es: ")
  print(100*sum(lhar.test == 5)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 6 en el test es: ")
  print(100*sum(lhar.test == 6)/nrow(lhar.test))
  
  
    
```  
  
  Podemos observar que no existe desbalance entre las seis clases contempladas, luego no podemos despreciar ninguna. 
  
  Aunque los datos están ya normalizados y escalados en [-1,1], estos presentan una gran asimetría como podemos observar:
  
```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    har_asymmetry <- apply(har.train, 2, skewness)
    har_asymmetry <- sort(abs(har_asymmetry), decreasing = T)
    print(head(har_asymmetry))
    
```
  
  Si representamos el atributo con mayor asimetría de todos obtenemos:

```{r, echo=F,warning=F}
  hist(har.train$V389, col = "blue")
```  

  Aunque los datos presentan una gran asimetría, no los quitamos porque "rompen el esquema".
  
  Además, la base de datos presenta un número demasiado alto de atributos, lo cual puede ser un gran problema a la hora utilizar métodos costosos para la predicción de las etiquetas. Con el objetivo de reducir el número de atributos sin perder información relevante, vamos a probar con dos métodos:
  
##Analisis de componentes principales (PCA)
  
  En primer lugar, probaremos a aplicar PCA a los datos con el objetivo de encontrar dependencias entre las variables y simplificar el conjunto de datos reduciendo los tiempos de cómputo. Los nuevos atributos considerados tras aplicar este método son combinaciones lineales de los atributos originales que recogen un tanto por ciento de la varianza acumulada de los datos pasado como argumento (*thresh*). Para la aplicación vamos a considerar *thresh = 0.95* ...
  
  Utilizaremos la función *preProcess* pasándole como argumento *method = c(pca)*.


```{r, echo=T, warning=F}
set.seed(123456789)
ObjetoTrans <- preProcess(har.train,method = c("pca"), thresh = 0.95)
```

```{r, echo=T, warning=F}
  print(ObjetoTrans)
```

Para mantener el 95\% de la varianza se necesitan mantener 102 atributos.

##Regresión LASSO (least absolute shrinkage and selection operator).

  Lasso es un método que lleva a cabo la regularización a la misma vez que realiza selección de características. 

El objetivo se basa en reducir el error de predicción, para ello, se ocupa de reducir la función:

\[
  R(\beta) = \sum_{i=1}^{n}(y_i - x_i\omega)^2 + \lambda \sum_{i=1}^{p}|\omega_i|
\]

donde $n$ es el número de muestras, $p$ el número de atributos y $w$ el vector de pesos solución. Así, obtiene diferentes valores de $\lambda$. Entre estos valores devueltos, podemos considerar dos de ellos:

- *lambda.min*, que nos devuelve el valor del $\lambda$ para el que se minimiza el error obtenido.
- *lambda.1se*. Aunque el $\lambda$ anterior sea menor, presenta una mayor dependencia de las particiones seleccionadas en la validación cruzada. La regla `1se` (one standard error), elige un valor de $\lambda$ para lo suficiente cercano a `lambda.min` para el cual el modelo obtenido sea lo suficiente simple, reduciendo así la dependencia del error respecto a las validaciones.

Podemos contemplar cualquiera de estos valores de $\lambda$ para regularizar nuestro modelo. 

Para obtener el modelo LASSO, realizaremos validación cruzada con tres particiones (dada la elevada dimensión de la base de datos). Tras obtener el modelo, considerando cada uno de los valores de *lambda* anteriormente comentado, obtenemos seis vectores de coeficientes donde cada uno de las componentes de estos vectores representa la relevancia de cada atributo para cada clase. Así, los atributos de los cuales podremos prescindir serán aquellos en los que la componente correspondiente a dicho atributo en todos los vectores sea nula. Realizando estas simplificaciones obtenemos que tras simplificar atributos las dimensiones obtenidas son:

```{r, echo=T, warning=F}
    set.seed(123456789)

    ml_lasso <- cv.glmnet(as.matrix(har.train), lhar.train[,1], family="multinomial", nfolds = 3)
    lambda.min <- ml_lasso$lambda.min
    lambda.1se <- ml_lasso$lambda.1se
    
    coeffs.min <- coef(ml_lasso, s = ml_lasso$lambda.min)
    coeffs.1se <- coef(ml_lasso, s = ml_lasso$lambda.1se)
    
    print(coeffs.min)
    print(coeffs.1se)
    
    #Eliminamos los atributos del train  

    coefficients.min <- matrix(ncol = 6, nrow = 562)
    coefficients.1se <- matrix(ncol = 6, nrow = 562)
    for(i in 1:6){
      coefficients.min[,i] <- (coeffs.min[[i]])[1:nrow(coeffs.min[[i]]),]
      coefficients.1se[,i] <- (coeffs.1se[[i]])[1:nrow(coeffs.1se[[i]]),]
    }
```
 
 
   
```{r, echo=T, warning=F}
    
    har.train_lasso.min = har.train[,abs(coefficients.min[1:6,])>0]
    har.train_lasso.1se = har.train[,abs(coefficients.1se[1:6,])>0]
    
    print("El número de atributos tras aplicar la reducción con lambda.min es: ")
    print(dim(har.train_lasso.min)[2])
    
    print("El número de atributos tras aplicar la reducción con lambda.1se es: ")
    print(dim(har.train_lasso.1se)[2])

```

Por tanto, aunque sería preferible aplicar una reducción de atributos basada en la regresión LASSO dado que los atributos obtenidos se corresponden con los atributos originales del modelo, obtenemos que tras comparar las tres reducciones de dimensionalidad la que mayor reducción produce es la aplicación de *PCA*. En conclusión, a partir de ahora trabajaremos con el conjunto de datos resultante de aplicar la reducción con *PCA*.


```{r, echo=T, warning=F}
har.trans.train <- predict(ObjetoTrans, har.train)
har.trans.test <- predict(ObjetoTrans, har.test)

har.train <- har.trans.train  
har.test <- har.trans.test
```

#Modelo lineal.

## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por la función `glm` (Generalized Linear Models) de `R`. Para cada familia, siempre que admitan, utilizaremos distintas funciones de enlace. Las funciones de enlace nos permiten establecer una relación entre la media de la respuesta y los predictores del modelo. Las familias que vamos a considerar para el ajuste son:

- **Binomial**, con link **logit**. Regresión logística.
- **Binomial**, con link **probit**. Modelo binomial, con función de enlace $\Phi^{-1}(\mu)$, donde $\Phi$ es la distribución acumulada de la distribución normal.
- **Binomial**, con link **cauchit**. Modelo binomial, cuya función de enlace es la análoga a la del modelo anterior sobre una distribución de Cauhy, en lugar de la normal.
- **Gaussiana**, con link **identity**. Regresión lineal.
- **Gaussiana**, con link **log**. Distribución normal con función de enlace logarítmica.
- **Poisson**. Distribución de Poisson. 
- **Quasi**. Este modelo no tiene una varianza determinada como en el resto de familias. Indicaremos la especificación de varianza `"constant"`
- **Quasibinomial.** Distribución binomial, con la única diferencia de que no fija el parámetro de dispersión (intenta describir varianza adicional en los datos que no puede ser explicada mediant una distribución binomial).
- **Quasipoisson.** Distribución de Poisson, con la única diferencia de que no fija el parámetro de dispersion.

```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){

  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.prob_test,ml.pred_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.prob_train,ml.pred_train))
}

testFamilies <- function(models, test, ltrain, ltest, modelNames = NULL){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  if(!is.null(modelNames)){
    rownames(results) <- modelNames
  }
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```



```{r, echo=F,warning=F}
  balance <- function(train, test, ltrain, ltest){
    #Juntamos datos con etiquetas
    train.aux <- cbind(train, ltrain)
    test.aux <- cbind(test, ltest)
    
    
    train.aux$ltrain <- as.factor(train.aux$ltrain)
    test.aux$ltest <- as.factor(test.aux$ltest)
    
    #Balanceamos
    train.aux <- SMOTE(ltrain~., train.aux, perc.over = 100, perc.under = 200)
    test.aux <- SMOTE(ltest~., test.aux, perc.over = 100, perc.under = 200)
    
    #Volvemos a separar datos de etiquetas
    train.aux$ltrain <- as.numeric(train.aux$ltrain)
    test.aux$ltest <- as.numeric(test.aux$ltest)
  
    
    train <- train.aux[,-ncol(train.aux)]
    ltrain <- train.aux[,ncol(train.aux)] - 1
  
    
    test <- test.aux[,-ncol(test.aux)]
    ltest <- test.aux[,ncol(test.aux)] - 1
    
    return(list(train,test,ltrain,ltest))
  }
```


```{r, echo=F,warning=F}
predictions1vs1 <- function(train, test, ltrain, ltest, modelNames = NULL){
  
  preds_ij <- matrix(nrow = 15, ncol = length(ltest))
  
  k <- 0
  
  for(i in 2:6){
    for(j in 1:(i-1)){
      cat(i," vs ",j,":\n")
      
      inds_i <- which(ltrain == i)
      inds_j <- which(ltrain == j)

      ltrain_i <- ltrain[inds_i]
      ltrain_j <- ltrain[inds_j]
      
      train_i <- train[inds_i,]
      train_j <- train[inds_j,]
      
      #test_inds_i <- which(ltest == i)
      #test_inds_j <- which(ltest == j)

      #ltest_i <- ltest[test_inds_i]
      #ltest_j <- ltest[test_inds_j]
      #test_i <- test[test_inds_i,]
      #test_j <- test[test_inds_j,]

      
      train_ij <- rbind(train_i,train_j)
      #test_ij <- rbind(test_i,test_j)
      ltrain_ij <- c(ltrain_i,ltrain_j)
      #ltest_ij <- c(ltest_i,ltest_j)
      
      
      
      ltrain_ij[ltrain_ij==j] <- 0
      ltrain_ij[ltrain_ij==i] <- 1
      #ltest_ij[ltest_ij==j] <- 0
      #ltest_ij[ltest_ij==i] <- 1

      
  
       ml.logit <- glm(ltrain_ij ~ ., family = binomial(logit), data = train_ij)

       predictions <- calculateEtest(ml.logit,test,ltest)[[3]]
       predictions[predictions==1] <- i
       predictions[predictions==0] <- j
      
       k <- k+1
       
       preds_ij[k,] <- predictions
    }
       
  }
  
  preds <- vector(length = length(ltest))
  for(i in 1:length(ltest)){
      preds[i] <- names(sort(table(preds_ij[,i]),decreasing=T))[1]#preds_ij[which(preds_ij[,i] == max(preds_ij[,i]))[1],i]
  }

  errorVec <- sum(preds != ltest)/length(ltest)
    
  return(list(preds,errorVec))  
}

```

```{r, echo=F,warning=F}
P <- predictions1vs1(har.train,har.test,as.vector(lhar.train[,1]),as.vector(lhar.test[,1]))

# Matriz de confusión
Eout.linear <- P[[2]]
confussion.linear <- table(P[[1]],lhar.test[,1])

Eout.linear
confussion.linear


```





#Redes Neuronales

Debido a las dimensiones de nuestra base de datos, formular una arquitectura de red neuronal con más de una capa o con muchas unidades en la capa oculta se hace inmanejable. Por ello, definimos nuestro modelo de red neuronal definido por una arquitectura con una única capa oculta y con un número fijo de unidades en la capa oculta igual a 5. Utilizaremos la función `neuralnet` proporcionada por R, de la que destacamos los siguientes parámetros:

- **hidden: ** Número de capas ocultas (hidden = 5).
- **threshold: ** Criterio de parada. Umbral entre las derivadas parciales de la función de error en dos iteraciones consecutivas. Lo dejaremos por defecto, 0.1.
- **stepmax: ** Número máximo de pasos. Estableceremos también este criterio de parada para evitar quedar atrapados en mínimos locales cuando usamos valores de tasa de aprendizaje muy bajas.
- **rep: ** Número de repeticiones que realizamos del aprendizaje de los pesos a partir de los datos de train. Realizaremos tres repeticiones para cada valor de la tasa de aprendizaje.
- **learningrate: ** Tasa de aprendizaje. Será el valor a estimar y tomará valores entre [0.8, 1.2]. Dependiendo de este valor, los saltos entre los pesos serán más o menos pronunciados. Se establecerá con un único valor en el intervalo dependiendo del ganador tras la estimación.
- **lifesign: ** Para establecer lo que queremos que imprima la función durante el aprendizaje de la red neuronal. Lo utilizaremos con dos valores diferentes: 'minimal' para el cálculo de la tasa de aprendizaje óptima y 'full' cuando analicemos más profundamente.
- **err.fct: ** Función de error usada para el cálculo del error. Usaremos la función por defecto, `sse`: suma de errores cuadráticos.
- **linear.output: ** Lo pondremos a *FALSE* para que interprete el problema como un problema de clasificación.


Como ya hemos comentado, para establecer el modelo calcularemos la tasa de aprendizaje óptima para la arquitectura de una capa con cinco unidades en la capa:


```{r, echo=F,warning=F}
  lhar.train.multi01 <- class.ind(lhar.train[,1])
  lhar.test.multi01 <- class.ind(lhar.test[,1])
  har.data.nnet <- cbind(har.train,lhar.train.multi01)

  names(har.data.nnet) <- c(names(har.data.nnet)[1:102], "l1", "l2", "l3", "l4", "l5", "l6")
  n <- names(har.data.nnet)[1:102]

# Obtenemos la fórmula:
  f.nnet <- as.formula(paste("l1 + l2 + l3 + l4 + l5 + l6 ~",paste(n[!n %in% c("l1","l2","l3","l4","l5","l6")], collapse = " + ")))


testingLearningRate <- function(formula, data,test, ltest){
    rates <- seq(from = 0.8, to = 1.2, 0.1)
    error <- vector()
    original.label <- max.col(ltest[,1])
    
    for(i in seq_along(rates)){
      ann <- neuralnet(formula = formula, data = data, hidden = 5, lifesign = 'minimal', learningrate = rates[i], rep = 3, linear.output = FALSE, stepmax = 10000)
      
      pr.nn <- compute(ann, test)
      pr.nn_ <- pr.nn$net.result
      pr.nn_2 <- max.col(pr.nn_)

      error[i] <- 1 - mean(pr.nn_2 == original.label)
    }
    
    return(error)
  }
  
```


```{r, echo=F,warning=F}
  set.seed(123456789)

  err.tasa <- testingLearningRate(f.nnet, har.data.nnet, har.test, lhar.test.multi01)
```

Si imprimimos el error producido para cada valor optenemos:



```{r, echo=F,warning=F}
```


#Máquina de Vectores Soporte

Las máquinas de vectores soporte son una de las técnicas más utilizadas para aprender distintos conjuntos de datos, por su simplicidad, su capacidad de maximizar márgenes y su facilidad para actuar en clasificaciones no lineales usando funciones kernel. Suelen proporcionar mejores resultados cuando los datos se reparten de forma homogénea y con poco ruido, permitiendo así controlar los márgenes de separación a partir de un conjunto pequeño de datos, los vectores soporte. Si el conjunto de datos es más heterogéneo, el ajuste será más complicado, puesto que la cantidad de vectores soporte puede aumentar, y serán más los vectores a adaptar. 

En nuestro caso, ya intuíamos previamente que los datos de las clases 1,2 y 3 pueden ser a priori fácilmente distinguibles de los de las clases 4,5 y 6, mientras que dentro de cada grupo de clases la separabilidad de los datos puede ser más complicada. Los vectores soporte es posible que se concentren en torno a los márgenes internos a cada grupo de clases. Según la capacidad de separación que tengan para los datos medidos los resultados para SVM serán mejores o no.


```{r, echo=F,warning=F}

tc <- tune.control(cross = 5)

svm_tune <- tune(svm, train.x = har.train, train.y = factor(lhar.train[,1]), kernel = "radial", ranges = list(gamma=c(0.01,0.1,1,10)), tunecontrol = tc)
```

Evaluamos sobre los datos test el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

svm_model <- svm(x = har.train, y = factor(lhar.train[,1]), kernel = "radial", gamma = 0.1)
summary(svm_model)


```

```{r, echo=F,warning=F}

svm.pred <- predict(svm_model,har.test)
Eout.svm <- sum(svm.pred != lhar.test[,1])/length(svm.pred)
confussion.svm <- table(svm.pred,lhar.test[,1])
  
```


#Boosting

```{r, echo=F,warning=F}

  #gbm_algorithm <- gbm(y ~ ., data = har.train, distribution = "adaboost", n.trees = 5000)
  blhar.train <- lhar.train[,1]
  blhar.test <- lhar.test[,1]
   
  boost_mat <- matrix(nrow = 6, ncol = length(blhar.test))
  
  for(i in 1:6){
  
    ltrain_i <- blhar.train
    ltest_i <- blhar.test
         
    ltrain_i[blhar.train != i] <- 0
    ltrain_i[blhar.train == i] <- 1
    ltest_i[blhar.test != i] <- 0
    ltest_i[blhar.test == i] <- 1
    
    boost_model <- gbm(ltrain_i ~ ., data = har.train, distribution = "adaboost", n.trees = 5000)
    
    boost_mat[i,] <- predict(boost_model,har.test,n.trees = 5000,type = "response" )
  
  }
  
  boost.pred <- vector(length = length(blhar.test))
  for(i in 1:length(blhar.test)){
      boost.pred[i] = which(boost_mat[,i]==max(boost_mat[,i]))[1]
  }
  
  Eout.adagbm <- sum(boost.pred!=blhar.test)/length(blhar.test)
  confussion.adabgm <- table(boost.pred,blhar.test)
  

```



```{r, echo=T, warning=F}
#sparsefactor -> true para aplicar regularización explicita norma L1 (?)
maboost_model <- maboost(x = har.train, y = as.factor(lhar.train[,1]), sparsefactor = TRUE)  

maboost.pred <- predict(maboost_model,har.test)
Eout.maboost <- sum(maboost.pred != lhar.test[,1])/length(maboost.pred)
confussion.maboost <- table(maboot.pred,lhar.test[,1])

```

#RandomForest

Random Forest es una técnica de aprendizaje basada e.n clasificadores simples utilizando bagging. Es  una técnica con una gran componente aleatoria, la cual permite obtener buenos resultados cuando trabajamos con datos más difusos o con más ruido. También puede ser bastante eficaz para clasificar problemas con múltiples etiquetas, aunque puede necesitar un gran número de ejemplos para que la aleatoriedad del algoritmo permita generalizar bien para la clasificación de nuevos datos.

En nuestro caso, disponemos tanto de un problema con múltiples etiquetas como de bastantes ejemplos, por lo que puede funcionar bien. En cuanto a la presencia de ruido, los datos provienen de medidas en un SmartPhone, que según su calidad, dichas medidas pueden tener distinta precisión, así que no podemos descartar que haya algo de ruido en los datos, si bien con el avance de la tecnología los instrumentos de medida son cada vez más precisos. En cualquier caso, Random Forest es una técnica para la cual los resultados sobre nuestro conjunto de datos pueden ser buenos.

A continuación nos planteamos definir un modelo con RandomForest. Utilizaremos la función `randomForest` proporcionada por R, de la que destacamos los siguientes parámetros:

- **ntree: ** Número de árboles considerados. Para estimar este número, realizaremos validación cruzada haciendo uso de la función `tune` proporiconada por R. 
- **mtry: ** Número de variables elegidas de forma aleatoria como candidatas para cada partición. Según los parámetros visto en teoría, para los problemas de clasificación el valor óptimo sería $sqrt(p)$ donde p sería el número de atributos considerados, por tanto, para nuestro problema *mtry = 10*. 

Para comprobar que se corresponde el valor óptimo con el valor establecido teóricamente, utilizamos la función `tuneRF` que nos devuelve el mejor valor de *mtry*. 

```{r, echo=F,warning=F}
    #La función tuneRF calcula a partir del valor por defecto de mtry el valor óptimo de mtry para el randomForest
    #Convertimos la etiqueta a un factor para que haga clasificación
    flhar.train <- as.factor(lhar.train[,1])
    best.mtry <- tuneRF(har.train, flhar.train, stepFactor = 1, improve = 0.02, ntree = 50)
```

```{r, echo=F,warning=F}
  print("El valor óptimo de mtry calculado es: ")
  print(best.mtry[,1])
```

Luego, el valor óptimo calculado del *mtry* se corresponde con el valor óptimo teórico. De aqui en adelante, utilizamos *mtry* = 10.

A continuación, experimentamos con el número de árboles óptimo para la definición de nuestro modelo de Random Forest final, consideraremos los valores de número árboles entre [100,1000]. Relizamos validación cruzada (5 folds) con la función tune y obtenemos lo siguiente:

```{r, echo=F,warning=F}
  set.seed(123456789)
   best.params <- tune(method = randomForest, har.train, flhar.train, ranges = list(ntree = c(100,200,300,400,500, 600, 700, 800, 900, 1000), mtry = 10), tunecontrol = tc)

```

El resultado de las pruebas realizadas ha sido:

```{r, echo=F,warning=F}
  print(best.params$performances)
```
```{r, echo=F,warning=F}
  print(best.params)
```


Como podemos observar, el número óptimo de áboles obtenido ha sido 800, aunque el error no ha variado mucho. 

Podemos observar el progreso del error en función del número de árboles utilizado en la siguiente gráfica:


```{r, echo=F,warning=F}
  points <- best.params$performances[c(1,3)]
  plot(points, type = "l", col = "blue")
```

<!--
Por regla general, cuanto mayor número de árboles se introduzcan al método, mejor será el error producido. Sin embargo, la mejora también decrece cuantos más árboles utilicemos. Por tanto, en cierto punto, el beneficio obtenido al aumentar el número de árboles será menor que el tiempo de cómputo necesitado para añadir estos árboles.
--->


Aunque podemos observar que las diferencias entre los errores obtenidos son pequeñas, como 800 árboles sigue siendo un número de árboles tratable en cuanto a tiempo de cómputo, elegimos este número como valor del parámetro *ntree*. Así, definimos nuestro modelo ganador de Random Forest con 800 árboles y *mtry = 10*.

Evaluamos sobre los datos test el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}
set.seed(123456789)

rf_model <- randomForest(x = har.train, y = flhar.train, ntree = 800, mtry = 10)
summary(rf_model)

```

```{r, echo=F,warning=F}

rf.pred <- predict(rf_model,har.test)
Eout.rf <- sum(rf.pred != lhar.test[,1])/length(rf.pred)
confussion.rf <- table(rf.pred,lhar.test[,1])
  
```

#Análisis de los resultados y conclusiones

```{r, echo=F,warning=F}

model_errors <- c(Eout.linear,Eout.svm,Eout.rf,Eout.adagbm,Eout.maboost)
model_confussions <- list(confussion.linear,confussion.svm,confussion.rf,confussion.adabgm,confussion.maboost)
model_names <- c("Lineal - Poisson", "SVM", "Random Forest","Adaboost - BGM", "Adaboost - MABoost")
names(model_errors) <- model_names
names(model_confussions) <- model_names
model_errors
model_confussions
```
