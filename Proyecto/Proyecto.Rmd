---
title: "PROYECTO FINAL: AJUSTE DE MODELOS NO-LINEALES"
author: "Nuria Rodríguez Barroso, Juan Luis Suárez Díaz."
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: pdf_document
toc: yes
---


\clearpage

```{r, echo = FALSE, warnings = FALSE, results = FALSE, message = FALSE}
    #Fijamos la semilla para obtener siempre los mismos resultados
    set.seed(123456789)

    #Añadimos librerías necesarias.
    library("caret")
    library("e1071")
    library("glmnet")
    library("leaps")
    library("DMwR")
    library("neuralnet")
    library("nnet")
    library("randomForest")
    library("gbm")
```


## Comprensión del problema a resolver: Human Activity Recognition Using Smartphones Data Set.

La base de datos elegida para el problema recoge la información de un grupo 30 voluntarios con edades comprendidas entre 18-48 años. Cada uno de estos individuos tenía que realizar una serie de actividades con el teléfono móvil enganchado en su cintura. Así, utilizando el dispositivo se podían registrar la aceleración lineal y angular en 3-ejes. Los experimentos se grabana en vídeo para poder etiquetar los datos de forma manual. De entre los datos recogidos, se seleccionaron de forma aleatoria el 70\% de esto para hacer de conjunto de training mientras que el otro 30\% se encargaba de recoger los datos para el test. 
Así, la base de datos se compone de 7352 muestras en el conjunto de train y 2947 en el conjunto de test, formando un total de 10299 muestras recogidas de estos 30 voluntarios y se consideran un total de 561 atributos.

Observamos el resumen de los diez primeros atributos:

```{r, echo = FALSE, warning=FALSE, results= FALSE, message = FALSE}
  
    #LECTURA DE LOS DATOS
    har.train <- read.table("./UCIHARDataset/train/X_train.txt", sep="", head = F)
    lhar.train <- read.table("./UCIHARDataset/train/y_train.txt", sep="", head = F)
    har.data.train <- cbind(lhar.train, har.train)
    
    har.test <- read.table("./UCIHARDataset/test/X_test.txt", sep="", head = F)
    lhar.test <- read.table("./UCIHARDataset/test/y_test.txt", sep="", head = F)
    har.data.test <- cbind(lhar.test, har.test)


    print(summary(har.train[,1:10]))
```

En cuanto a las etiquetas, nos encontramos ante un problema de clasificación multiclase, dado que las etiquetas toman valores enteros en el intervalo [1,6]. Cada etiqueta clasifica el movimiento registrado por el dispositivo móvil y lo clasifica en seis tipos:
  1. WALKING
  2. WALKING_UPSTAIRS
  3. WALKING_DOWNSTAIRS
  4. SITTING
  5. STANDING
  6. LAYING

Por tanto, para la aplicación de aquellos métodos que precisen de clasificación binaria para un buen funcionamiento utilizaremos la técnica de *one vs all*, la cual consistirá en dividir nuestro problema de clasificación multiclase en seis problemas de clasificación binaria de la forma: clase_i VS no_clase_i. 

Así, el cálculo del error para cada modelo propuesto consistirá en el número de errores cometidos. Esto es, número de muestras con etiqueta asignada de forma errónea. Para aquellos modelos que precisen de una adaptación a subproblemas de clasificación binaria, obtendremos seis vectores con la probabilidad de asignación de cada una de las seis etiquetas disponibles. Por tanto, le asignaremos a cada elemento la etiqueta que obtenga una mayor probabilidad, calculando el error en este caso de la misma forma que para el resto de modelos.

## Preprocesado de datos.

  En primer lugar, observamos que nuestra base de datos no contiene valores perdidos (NaNs), por lo cual no será necesario el trato especial de estos valores. Lo mismo ocurre con las variables cualitativas.
  
  En cuanto al desbalanceo, observemos el porcentaje de elementos de cada clase en el train y después en el test:

```{r, echo=F,warning=F}
  #Estudiamos si hay desbalanceo
  print("El porcentaje de elementos con etiqueta 1 en el train es: ")
  print(sum(lhar.train == 1)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 2 en el train es: ")
  print(sum(lhar.train == 2)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 3 en el train es: ")
  print(sum(lhar.train == 3)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 4 en el train es: ")
  print(sum(lhar.train == 4)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 5 en el train es: ")
  print(sum(lhar.train == 5)/nrow(lhar.train))
  print("El porcentaje de elementos con etiqueta 6 en el train es: ")
  print(sum(lhar.train == 6)/nrow(lhar.train))
```


```{r, echo=F,warning=F}

  print("El porcentaje de elementos con etiqueta 1 en el test es: ")
  print(sum(lhar.test == 1)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 2 en el test es: ")
  print(sum(lhar.test == 2)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 3 en el test es: ")
  print(sum(lhar.test == 3)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 4 en el test es: ")
  print(sum(lhar.test == 4)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 5 en el test es: ")
  print(sum(lhar.test == 5)/nrow(lhar.test))
  print("El porcentaje de elementos con etiqueta 6 en el test es: ")
  print(sum(lhar.test == 6)/nrow(lhar.test))
  
  
    
```  
  
  Podemos observar que no existe desbalance entre las seis clases contempladas, luego no podemos despreciar ninguna. 
  
  PREGUNTAR AQUI CON CUANTAS DESBALANCEAMOS PORQUE:
  
  Ahora bien, a la hora de realizar los subproblemas *one vs all* nos encontraremos con un balanceo aproximado del 20\% - 80\% al 15\%-85\%, entonces -> No hay que aplicar desbalanceo, o sip, no ze zabe.
  
  Aunque los datos están ya normalizados y escalados en [-1,1], estos presentan una gran asimetría como podemos observar:
  
```{r, echo=F,warning=F}
    #Eliminación de variables con varianza 0 o muy próximas (importante para métodos sensibles a distancias)
   
   #Ordenamos las columnas por asimetria
    har_asymmetry <- apply(har.train, 2, skewness)
    har_asymmetry <- sort(abs(har_asymmetry), decreasing = T)
    print(head(har_asymmetry))
    
```
  
  Si representamos el atributo con mayor asimetría de todos obtenemos:

```{r, echo=F,warning=F}
  hist(har.train$V389, col = "blue")
```  

  
  Esta asimetría deberá ser tratada con el método BoxCox.
  
  Además, la base de datos presenta un número demasiado alto de atributos, lo cual puede ser un gran problema a la hora utilizar métodos costosos para la predicción de las etiquetas. Por ello, aplicaremos PCA a los datos con el objetivo de encontrar dependencias entre las variables y simplificar el conjunto de datos reduciendo los tiempos de cómputo.
  
  Para realizar estas dos operaciones de procesamiento, utilizaremos la función *preProcess* pasándole como argumento *method = c(BoxCox, pca)*. Como ya habíamos comentado anteriormente, aunque los datos estaban previamente centrados y escalados, al aplicar estos dos métodos de preprocesado se ha perdido esta propiedad, por tanto, aplicaremos también centrado y escalado a los datos. Luego, la llamada al método quedaría de la siguiente forma:

<!--  
#0.95 -> 102 componentes
#0.9 -> 63 componentes
#0.85 -> 40 componentes
#0.8 -> 26 componentes
#0.75 -> 16 componentes
#0.7 -> 10 componentes


??? <- CON CUANTOS NOS QUEDAMOS AL FINAL?

-->  
```{r, echo=T, warning=F}

ObjetoTrans <- preProcess(har.train,method = c("pca", "BoxCox", "center", "scale"), thres = 0.8)
```

```{r, echo=T, warning=F}

har.trans.train <- predict(ObjetoTrans, har.train)
har.trans.test <- predict(ObjetoTrans, har.test)

har.train <- har.trans.train  
har.test <- har.trans.test

```


#Modelo lineal.





## Selección de clases de funciones a usar

Los modelos que vamos a intentar ajustar son los proporcionados por la función `glm` (Generalized Linear Models) de `R`. Para cada familia, siempre que admitan, utilizaremos distintas funciones de enlace. Las funciones de enlace nos permiten establecer una relación entre la media de la respuesta y los predictores del modelo. Las familias que vamos a considerar para el ajuste son:

- **Binomial**, con link **logit**. Regresión logística.
- **Binomial**, con link **probit**. Modelo binomial, con función de enlace $\Phi^{-1}(\mu)$, donde $\Phi$ es la distribución acumulada de la distribución normal.
- **Binomial**, con link **cauchit**. Modelo binomial, cuya función de enlace es la análoga a la del modelo anterior sobre una distribución de Cauhy, en lugar de la normal.
- **Gaussiana**, con link **identity**. Regresión lineal.
- **Gaussiana**, con link **log**. Distribución normal con función de enlace logarítmica.
- **Poisson**. Distribución de Poisson. 
- **Quasi**. Este modelo no tiene una varianza determinada como en el resto de familias. Indicaremos la especificación de varianza `"constant"`
- **Quasibinomial.** Distribución binomial, con la única diferencia de que no fija el parámetro de dispersión (intenta describir varianza adicional en los datos que no puede ser explicada mediant una distribución binomial).
- **Quasipoisson.** Distribución de Poisson, con la única diferencia de que no fija el parámetro de dispersion.



```{r, echo=F,warning=F}
calculateEtest <- function(ml, test, ltest, s=0){
  #Cálculo de probabilidades
  if(s==0){
    ml.prob_test = predict(ml, test, type="response")
  }else{
    ml.prob_test = predict(ml,as.matrix(test),type = "response", s = s)
  }

  # Etest
  ml.pred_test = rep(0, length(ml.prob_test)) # predicciones por defecto 0
  ml.pred_test[ml.prob_test >=0.5] = 1 # >= 0.5 clase 1
  

  ml.Etest = mean(ml.pred_test != ltest)
  
  return(list(ml.Etest, ml.prob_test))
}

calculateEin <- function(ml ,ltrain){
  ml.prob_train = predict(ml, type = "response") #no tenemos que introducirle el train porque recuerda
  # Ein
  ml.pred_train = rep(0, length(ml.prob_train)) #predicciones por defecto 0
  ml.pred_train[ml.prob_train >= 0.5] = 1
    
  ml.Ein = mean(ml.pred_train != ltrain)

  return(list(ml.Ein, ml.prob_train))
}

testFamilies <- function(models, test, ltrain, ltest, modelNames = NULL){

  results <- matrix(nrow=length(models), ncol = 2)
  colnames(results) <- c("Ein","Eout")
  if(!is.null(modelNames)){
    rownames(results) <- modelNames
  }
  for(i in seq_along(models)){
    results[i,1] <-calculateEin(models[[i]],ltrain)[[1]]
    results[i,2] <- 
      calculateEtest(models[[i]],test,ltest)[[1]]
  }
  
  return(results)
}
```

```{r, echo=F,warning=F}
  balance <- function(train, test, ltrain, ltest){
    #Juntamos datos con etiquetas
    train.aux <- cbind(train, ltrain)
    test.aux <- cbind(test, ltest)
    
    
    train.aux$ltrain <- as.factor(train.aux$ltrain)
    test.aux$ltest <- as.factor(test.aux$ltest)
    
    #Balanceamos
    train.aux <- SMOTE(ltrain~., train.aux, perc.over = 100, perc.under = 200)
    test.aux <- SMOTE(ltest~., test.aux, perc.over = 100, perc.under = 200)
    
    #Volvemos a separar datos de etiquetas
    train.aux$ltrain <- as.numeric(train.aux$ltrain)
    test.aux$ltest <- as.numeric(test.aux$ltest)
  
    
    train <- train.aux[,-ncol(train.aux)]
    ltrain <- train.aux[,ncol(train.aux)] - 1
  
    
    test <- test.aux[,-ncol(test.aux)]
    ltest <- test.aux[,ncol(test.aux)] - 1
    
    return(list(train,test,ltrain,ltest))
  }
```

```{r, echo=F,warning=F}
predictions <- function(train, test, ltrain, ltest, modelNames = NULL){
  
  M <- matrix(nrow = 9, ncol = length(ltest))
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial1 <- glm(ltrain_i ~ ., family = binomial(logit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[1,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial2 <- glm(ltrain_i ~ ., family = binomial(probit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial2,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[2,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.binomial3 <- glm(ltrain_i ~ ., family = binomial(cauchit), data = train)
       
       preds_j[i,] <- calculateEtest(ml.binomial3,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[3,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.gaussian1 <- glm(ltrain_i ~ ., family = gaussian(identity), data = train)
       
       preds_j[i,] <- calculateEtest(ml.gaussian1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[4,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.gaussian2 <- glm(ltrain_i ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
       
       preds_j[i,] <- calculateEtest(ml.gaussian2,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[5,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.poisson1 <- glm(ltrain_i ~ ., family = poisson(log), data = train)
       
       preds_j[i,] <- calculateEtest(ml.poisson1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[6,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.quasi1 <- glm(ltrain_i ~ ., family = quasi(link = "identity", variance = "constant"), data = train)
       
       preds_j[i,] <- calculateEtest(ml.quasi1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[7,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
        ml.quasibinomial1 <- glm(ltrain_i ~ ., family = quasibinomial(link = "logit"), data = train)
       
       preds_j[i,] <- calculateEtest(ml.quasibinomial1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[8,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  preds_j <- matrix(nrow = 6, ncol = length(ltest))
  for(i in 1:6){
      #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       
       
       ml.quasipoisson1 <- glm(ltrain_i ~ ., family = quasipoisson(link = "log"), data = train) 
       
       preds_j[i,] <- calculateEtest(ml.quasipoisson1,test,ltest)[[2]]
       
  }
  
  for(i in 1:length(ltest)){
      M[9,i] <- which(preds_j[,i]==max(preds_j[,i]))[1]
  }
  
  
  
  
  
  
  
  
  
  #
  #gaussiano
  #
  #
  #poisson
  #
  
  #quasi
  #

  #quasibinomial
  #
  #quasipoisson
  # 
  #models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
    #modelNames <- c("Gaussian","Poisson")
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  rownames(M) <- modelNames
  
  errorVec <- vector(length = 9)
  for(i in 1:9){
      errorVec[i] <- sum(M[i,]!=ltest)/length(ltest)
  }
  names(errorVec) <- modelNames
    
  return(list(M,errorVec))  
}

```

```{r, echo=F,warning=F}
P <- predictions(har.train,har.test,lhar.train[,1],lhar.test[,1])

# Matriz de confusión para Poisson (que proporciona el menor error)
table(P[[1]][6,],lhar.test[,1])


```

```{r, echo=F,warning=F}
validateLinearModels <- function(train,test,ltrain,ltest){
  count_1 = sum(ltrain==1)
  count_0 = sum(ltrain==0)
  count = count_1 + count_0
  
  if(count_1/count <= 0.1 || count_0/count <= 0.1){
      l <- balance(train, test, ltrain, ltest)
      train <- l[[1]]
      test <- l[[2]]
      ltrain <- l[[3]]
      ltest <- l[[4]]
  }
  
  #Modelos
  #binomial
  ml.binomial1 <- glm(ltrain ~ ., family = binomial(logit), data = train)
  ml.binomial2 <- glm(ltrain ~ ., family = binomial(probit), data = train)
  ml.binomial3 <- glm(ltrain ~ ., family = binomial(cauchit), data = train)
  #gaussiano
  ml.gaussian1 <- glm(ltrain ~ ., family = gaussian(identity), data = train)
  ml.gaussian2 <- glm(ltrain ~ ., family = gaussian(log), data = train, start=rep(0, ncol(train)+1))
  #poisson
  ml.poisson1 <- glm(ltrain ~ ., family = poisson(log), data = train)
  
  #quasi
  ml.quasi1 <- glm(ltrain ~ ., family = quasi(link = "identity", variance = "constant"), data = train)

  #quasibinomial
  ml.quasibinomial1 <- glm(ltrain ~ ., family = quasibinomial(link = "logit"), data = train)
  #quasipoisson
  ml.quasipoisson1 <- glm(ltrain ~ ., family = quasipoisson(link = "log"), data = train)  
  models <- list(ml.binomial1, ml.binomial2, ml.binomial3, ml.gaussian1, ml.gaussian2, ml.poisson1, ml.quasi1, ml.quasibinomial1, ml.quasipoisson1)
    #modelNames <- c("Gaussian","Poisson")
    modelNames <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
  testFamilies(models, test, ltrain, ltest, modelNames)
  
}


```




Una vez definidos los modelos y las funciones a usar, procedemos al ajuste de los distintos modelos y al análisis de sus errores:

```{r, echo=F,warning=F}
validateLinearAllLabels <- function(train,test,ltrain,ltest){
    output <- matrix(nrow = 9, ncol = 12)
    rownames(output) <- c("Binomial - Logit", "Binomial - Probit", "Binomial - Cauchit", "Gaussian - Identity", "Gaussian - Log", "Poisson", "Quasi", "Quasibinomial","Quasipoisson")
    colnames(output) <- c("Ein 1", "Eout 1","Ein 2", "Eout 2","Ein 3", "Eout 3","Ein 4", "Eout 4","Ein 5", "Eout 5","Ein 6", "Eout 6")
    for( i in 1:6){
       #cat("Comparación ",i," vs  NO ",i,":\n")
       ltrain_i <- ltrain
       ltest_i <- ltest
       
       ltrain_i[ltrain != i] <- 0
       ltrain_i[ltrain == i] <- 1
       ltest_i[ltest != i] <- 0
       ltest_i[ltest == i] <- 1
       
       M <- validateLinearModels(train,test,ltrain_i,ltest_i)
       
       output[,c(2*i-1,2*i)] <- M[,c(1,2)]
    } 
    output
}
  
```

```{r, echo=F,warning=F}
validateLinearAllLabels(har.train,har.test,as.vector(lhar.train[,1]),as.vector(lhar.test[,1]))

```

#Redes Neuronales

En primer lugar aplicamos validación cruzada para optimizar:
- size: número de unidades ocultas intermedias
- decay: se usa para evitar el sobre ajuste

Como queremos usarlo para clasificación debemos usar el parámetro: lineout = T.

```{r, echo=F,warning=F}
# Controlador de tune. Debido al tamaño de los datos, utlizaremos validación cruzada 5-fold para la estimación de hiperparámetros.
tc <- tune.control(cross = 5)
```


```{r, echo=F,warning=F}
  #train.data <- cbind(har.train, lhar.train)
```

Lo hacemos hasta 25 porque peta. La función tune obtiene por validación cruzada el error para un tamaño de capa oculta prefijada.

```{r, echo=F,warning=F}
  tuneNNet <- function(data, label){
    #Probamos los parámetros con tune.nnet
    for(i in 1:30){
        s[i] <- (tune.nnet(x = data, y = label, size = i))
    }
    
    s <- cbind(c(1:30), s)
    s
  }
  
```

Transformación de etiquetas para `nnet`. Para evitar tratar con datos categóricos (etiquetas $1,\dots,6$), que no pueden ser tratados con `nnet`, creamos 6 columnas de etiquetas a 0 o 1.

```{r, echo=F,warning=F}
lhar.train.multi01 <- class.ind(lhar.train[,1])
har.data.nnet <- cbind(har.train,lhar.train.multi01)

names(har.data.nnet) <- c("V1","V2","V3","V4","V5","V6","V7","V8","V9","V10","l1","l2","l3","l4","l5","l6")
n <- names(har.data.nnet)[1:10]

# Obtenemos la fórmula:
f.nnet <- as.formula(paste("l1 + l2 + l3 + l4 + l5 + l6 ~",paste(n[!n %in% c("l1","l2","l3")], collapse = " + ")))

```

```{r, echo=F,warning=F}
  #tuneNNet(har.train, lhar.train)

tune_nnet <- tune(nnet,train.x = f.nnet, data = lhar.data.nnet, ranges = list(size = 1:50),tunecontrol = tc)
```

Predecimos el error sobre la muestra test para el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

nnet_model <- nnet(formula = f.nnet, data = lhar.data.nnet, size = 1)
summary(nnet_model)


```

```{r, echo=F,warning=F}

har.test.nnet <- har.test
names(har.test.nnet) <- c("V1","V2","V3","V4","V5","V6","V7","V8","V9","V10")
#nnet.pred <- predict(nnet_model,har.test.nnet, type = "raw")
# No sé por qué sale todo 1 aquí

#compute(nnet_model,har.test.nnet)
```

El número de unidades óptimo para una sola capa oculta es: 19. 

Probamos ahora entre las redes neuronales de:
  - Una capa con 19 unidades
  - Dos capas con 19 unidades cada una.
  - Tres capas con 19 unidades cada una.
  
  Vamos a utilizar la validación cruzada de mhe
  
   = data.aux[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
  
```{r, echo=F,warning=F}

  chooseNumberLayers <- function(data, label, size){
    #data.aux<-cbind(data,label)
    #linear.output -> True para clasificacion
    #algorithm -> por defecto rprop+ (?)

    per <- sample(nrow(data), nrow(data))
    #Barajamos
    data[per,]

    #Calculamos error con una capa
    m <- matrix(nrow = 3, ncol = 1)
  
    for(j in 1:3){
       error <- 0
       for(i in 1:5){
          #Esto cogido de internet -> CUIDADO
          train <- data[((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5),]
          ltrain <- label[((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5),]
          n <- names(train)
          f <- as.formula(paste("ltrain ~", paste(n[!n %in% "ltrain"], collapse = " + ")))
          m <- neuralnet(f, train , hidden = rep(size,j), linear.output = TRUE)
          
          #No estoy segura de como calcular el error 
          ltest <- label[-(((i-1)*nrow(label)/5 + 1):((i)*nrow(label)/5)),]
          test <- data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),]
          pnn.m <- compute(m, data[-(((i-1)*nrow(data)/5 + 1):((i)*nrow(data)/5)),])  
          pr.nn_ <- pnn.m$net.result*(max(ltrain)-min(ltrain))+min(ltrain)
          test.r <- (ltest)*(max(ltrain)-min(ltrain))+min(ltrain)

          error < error  + sum((test.r - pr.nn_)^2)/nrow(test)
       }   
       
       m[j] <- error/5
    }

    
  }

```


```{r, echo=F,warning=F}
  chooseNumberLayers(har.train, lhar.train, 19)
```

#Máquina de Vectores Soporte

- Los datos están normalizados.

```{r, echo=F,warning=F}

tc <- tune.control(cross = 5)

svm_tune <- tune(svm, train.x = har.train, train.y = factor(lhar.train[,1]), kernel = "radial", ranges = list(gamma=c(0.01,0.1,1,10)), tunecontrol = tc)
```

Evaluamos sobre los datos test el mejor modelo obtenido en la validación cruzada.

```{r, echo=F,warning=F}

svm_model <- svm(x = har.train, y = factor(lhar.train[,1]), kernel = "radial", gamma = 0.1)
summary(svm_model)


```

```{r, echo=F,warning=F}

svm.pred <- predict(svm_model,har.test)
Eout.svm <- sum(svm.pred != lhar.test[,1])/length(svm.pred)
table(svm.pred,lhar.test[,1])
  
```


#Boosting

```{r, echo=F,warning=F}

  #gbm_algorithm <- gbm(y ~ ., data = har.train, distribution = "adaboost", n.trees = 5000)
  #gbm_algorithm <- gbm

```


#RandomForest



Hay dos parámetros importantes para predecir:
1- Número de árboles (ntree)
2- Número de variables aleatorias usadas en cada árbol (mtry)

Calculamos el número de variables aleatorias usadas en cada árbol óptimo:

```{r, echo=F,warning=F}
    #La función tuneRF calcula a partir del valor por defecto de mtry el valor óptimo de mtry para el randomForest
    #Convertimos la etiqueta a un factor para que haga clasificación
    flhar.train <- as.factor(lhar.train[,1])
    best.mtry <- tuneRF(har.train, flhar.train, stepFactor = 1, improve = 0.02, ntree = 500)
```

-> Es sqrt(9) que es el valor optimo en teoria.


Calculamos el número de árboles óptimo:

```{r, echo=F,warning=F}
   best.params <- tune(method = randomForest, har.train, flhar.train, ranges = list(ntree = c(10,20,30), mtry = 3))

```

